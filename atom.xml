<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[linpingta's blog]]></title>
  <link href="http://yoursite.com/atom.xml" rel="self"/>
  <link href="http://yoursite.com/"/>
  <updated>2020-06-24T10:13:34+08:00</updated>
  <id>http://yoursite.com/</id>
  <author>
    <name><![CDATA[linpingta]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Facebook-RTB广告投放及策略工程师工作]]></title>
    <link href="http://yoursite.com/blog/2016/10/29/facebook-strategy-thinking/"/>
    <updated>2016-10-29T11:50:23+08:00</updated>
    <id>http://yoursite.com/blog/2016/10/29/facebook-strategy-thinking</id>
    <content type="html"><![CDATA[<p>我希望这是一篇有意思的博客，虽然内容里没有代码，没有技术名词。我想说说Facebook广告投放是怎么回事，然后说说作为策略工程师的一些感受。
首先说明，策略的内容不能谈，工作里确实见过不少，每个广告主都有不同的想法，我们也有自己的想法，但一是直接把这些放在这里不合适，毕竟不都是我的想法，二是看起来没有必胜的策略，所以如果想看这个就请忽略本文吧。</p>

<h3>Facebook广告</h3>

<p>因为我是做这个的，先谈谈对自动管理系统的认识：</p>

<pre><code>我觉得一直存在一个误区，好像用了自动投放系统，效果就能特别好，量级又大成本又低。其实不是的，广告的效果本身是取决于产品（魔兽就是比一个不知名的MMORPG好投），其次取决于创意（美女图片ctr就是比一般产品高），自动投放系统只是控制广告的预算出价和开关，它没有能力把一个坏创意或者坏产品变成好的，它能做的，其实也只是在广告量价关系上的改善：把好创意尽量多投（高出价高预算），把坏创意尽量少投（及早关停）。
</code></pre>

<p>下面说正经的：</p>

<h4>原理篇</h4>

<p>Facebook有四种竞价方式：CPM, CPC, oCPM和CPA。前两种分别是对M（展现）和C（点击）出价，后两种都是为A（激活）出价，但结算方式不同。
从覆盖人群能力讲，CPM>CPC>oCPM>CPA，因为CPC相比CPM要做ctr的预估，这就会过滤到一批不合理的人群，oCPM与CPC同理。
通常而言，App广告主都会选择oCPM或CPA，但CPA要求广告账户有足够的安装积累为前提，因此并非一开始就可以用，所以oCPM相比而言用的最多。但题外一句，CPA对成本控制会更好。
但无论是哪种竞价方式，Facebook在内部都是会进行广告排序，然后决定对这个流量展现哪个广告，而广告排序，就必然要统一到CPM级别，CPC广告会根据预估的ctr和当前的C单价去计算一个CPM，oCPM和CPA则是要预估ctr和cvr，然后反算一个CPM。
Facebook的广告排序不仅考虑CPM（即谁的出价高），同时也要考虑广告的品质，即质量得分（relevance score），当用户Like转发品论广告时，relevance score会提高，反之当他们关闭广告时，relevance score会降低。
最终，每个广告会获得一个分数，这个分数取决于CPM和relevance score</p>

<pre><code>score = CPM + a * relevance_score
</code></pre>

<p>当然，尽管Facebook对外暴露出的relevance score只有1-10，但它内部肯定有更详细的指标，只是我们可以把relevance score作为一个定性指标了。
从这个角度出发，提高广告曝光有两个途径：提高广告质量， 提高广告出价（无论是A,C，还是M）。
Facebook还有一个因素是daily budget，因为pacing算法的存在，daily budget其实也是对广告花费量级有影响的。</p>

<h4>指标篇</h4>

<p>当广告投起来后，我们就会看到展现，点击，安装，常用的几个指标：ctr, cvr, cpm, cpa, spend, roi</p>

<pre><code>ctr：click/impression，点击率太低的广告，要想想素材第一眼的吸引力
cvr：install/click，转化率太低的广告，可能主要是素材和产品不匹配，用户点进去了发现不是那么回事
cpm：1000*spend/impression，其实表示你覆盖的人群类别，
cpa：成本，广告主关心问题之一
spend：花费，广告主关心问题值二
roi：(income-spend)/spend，其实就是由cpa和spend决定的
</code></pre>

<p>这些指标，重要的不仅是绝对值，还有相对值：</p>

<pre><code>和昨天同时段的比较
</code></pre>

<p>通常而言，CPA在投放地域的凌晨都会很高，而量级都会很少，以东南亚为例，我们通常都是在白天看数据，那么直接比较今天和昨天的CPA并不信服，而是应该看看今天和昨天同时段的情况。同理，周末的成本也会和平时不一样，这点也需要在比较时参考。</p>

<pre><code>和理论值的比较
</code></pre>

<p>CPA在不同投放地域的表现不同，欧美的成本就是远高于东南亚，cvr对新游戏老游戏不同，在投放地域存在基准值，可以用于判断ctr和cvr是否合理，以及素材本身是否需要改进。</p>

<pre><code>看趋势
</code></pre>

<p>有些时候成本或量级不符合要求，我们做了某些调整，但如果只看总体的效果，一段时间后仍然不符合预期，这时候需要仔细去分析小时数据的变化，即调整后怎么样了，才能合理评估调整的效果。这里面的一个黑科技是，通常调整后的一个小时，成本都有可能飙高，所以一是不要调整太频繁，二是看3-4个小时的效果较好。</p>

<p>再说说绝对值的事情：</p>

<pre><code>要看花费后再看CPA/ctr/cvr
</code></pre>

<p>花费只是几十美金的情况下，看CPA/ctr/cvr的绝对值高低参考意义不大。一定量级后再分析成本，包括离线模型也会滤除花费太少的ad。</p>

<pre><code>CPM与ctr*cvr是正相关的
</code></pre>

<p>投出的CPM某种程度上代表了你买到的人群，抛去特定产品的因素，人群本身对游戏的转化率也是不同的，比如有些人群会更喜欢玩游戏，当然，对他们的竞争也更激烈，表现为CPM也更高。
所以这里的一个黑科技是，覆盖不同的人群，因为本质上说，CPM增加和ctr*cvr增加速率高低，决定了CPA的大小，所以如果能够在指定人群上取得增量收益，即虽然花的钱多，但ctr*cvr增加更多，你还是能够降低成本的。</p>

<h4>设置篇</h4>

<p>素材</p>

<pre><code>做好素材，这不用多说了，原则上你素材够好，下面我写的都可以不用看了

多尝试不同的素材类型，比如轮播，视频，尤其是新广告形式刚刚上线的时候
文案意义不大，从目前的情况看，把语言搞对就行了
</code></pre>

<p>定向</p>

<pre><code>不要在一个定向上设置多于4个创意，否则也会竞争
不要在一个地域定向里包含太多国家，除非有必要这样做（比如每个国家人群太小）
对太大的游戏设置wifi-only
设置排除app和page like
一版friend of connection没什么用
游戏性别基本就是男性，应用可以混着
受众比较有用
兴趣其实用一般都名词就好，太细了除非你有研究
。。。
保证你的受众人群不要太小，越小竞争肯定越激烈
</code></pre>

<p>设置</p>

<pre><code>bid方法很多，我很推荐用我们的系统，简单来说，就按广告主的设置也可以，或者了解投放地域特点大致设置
daily budget 如果是应用，不要太高，如果是游戏，就看要不要量级
pause/start 创意级别开关可以的，但最好不要只看凌晨的数据
autobid: 有些时候结合daily budget可以，有些时候测试图不错，有些时候也会飞
</code></pre>

<h3>策略工程师随想</h3>

<p>其实最开始到RTB项目组，我们日常最花费精力的事情就是投放广告case的维护，现在想想，这应该是策略工程师的日常。</p>

<pre><code>运营会反馈：“我的广告投不出去了”，“我的广告成本不行”，“我的广告花费速度比昨天慢了”，“我的广告被拒了”。。。
</code></pre>

<p>策略工程师的任务
第一步来说，是通过后台投放的逻辑和数据分析，去发现问题究竟出在哪里。</p>

<pre><code>比如广告花费少，是因为内部排序的win\_rate低，那么是不是这个case单价过低，比如广告内部win\_rate不低，但在Ad Exchange上的win\_rate低，那么对比几天，看看是不是有土豪对手在竞价。。。再比如成本相差大，看看是不是ctr预估不准，是离线的问题，能不能加在线的因素控制，是在线的问题，看看会不会是时间窗口问题。。。
</code></pre>

<p>第二步，是根据发现的情况，给运营建议。因为在RTB投放里，运营看到的是一个黑盒系统，策略工程师实现系统，他可以了解问题出在哪里。</p>

<pre><code>比如是创意不行，还是出价过低，定向人群过少，或者其它的一些设置不合理（初始探测预算较小），也有可能问题是系统本身的问题，简单说是bug，复杂说可能是系统现在的局限。比如新创意没有保护，是否考虑保护一段时间，至少让它有一定的展现，万一是ctr, cvr很高的case呢
</code></pre>

<p>第三步，是总结常见的问题，进行系统改进。策略工程师相比运营更加理性，同时可以了解更多运营的通用需求，这种情况下，去总结问题，将它变成策略。
策略要注意的是实验，条件允许的情况下，总要通过A/B测试去确定策略是否合理。
策略同样要注意case by case的特点，广告主的诉求彼此不同，通常在投放的不同阶段也不会相同，所以策略总是有使用条件的，甚至有些时候，为特定广告主特定调整参数，也是完全可能的。</p>

<p>第四步，和前三步不同，那就是问题未必来自运营的直接反馈，而是自己的主动发现。
<strong>广告的核心问题，就是量级和成本的关系。我们总是希望用少的成本，同时获取高的量级。</strong></p>

<pre><code>当然它们本身是矛盾的，但总有一个平衡点，比如广告的动态出价，依赖当前广告的实际成本，去反馈不一样的出价，当前成本高了，实际出价低些，反之高些
</code></pre>

<p>这些大概都是两年以前在RTB项目组做项目的体会，现在居然还有不少印象。我在RTB项目组只呆了半年的时间，后来新成立Facebook项目组，我是从零开始的参与，事情类别多了很多，比如要先开始搭广告管理和统计的服务，研究Facebook API，后来又做数据支持和应用，但很大一部分工作，仍然是在策略上。
Facebook的模式和RTB不太一样，RTB是对每个流量竞价，你有个内部广告库，要过滤后排序，然后给出一个流量的绝对出价；Facebook你看不到每个流量，也不需要你对广告自己做排序，但要你为每个定向决定出价。从模式上说，Facebook很像做SEO，从技术上说，Facebook模式里，我们扮演的是RTB运营的角色，内部广告系统对我们是黑盒，我们要决定该怎么参与其中。
关于Facebook投放中的一些指标，在上面已经提了，总体而言，因为Facebook的黑盒特点，策略工程师的事情更不容易做，但我也在想办法更好的去理解它。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[saiku安装及基础配置]]></title>
    <link href="http://yoursite.com/blog/2016/10/25/saiku-install/"/>
    <updated>2016-10-25T11:56:21+08:00</updated>
    <id>http://yoursite.com/blog/2016/10/25/saiku-install</id>
    <content type="html"><![CDATA[<h4>背景</h4>

<p>saiku是一个开源的OLAP分析平台，在我们的项目中逐渐发现，除了基础的数据报表需求，以及数据挖掘需求外，也需要考虑把部分查询交互起来，一方面可以减少报表方面的压力，另外一方面也可以快速对数据进行验证，毕竟通过界面拖拽的方法相比直接写SQL还是要方便很多的。
在公司的另外一个项目里，saiku已经有比较成熟的应用，因此我们选择saiku作为我们需求处理的工具（尽管客观说saiku社区版还是有自己的问题的）。</p>

<h4>文档地址</h4>

<pre><code>http://wiki.meteorite.bi/display/SAIK/Saiku+Features
</code></pre>

<h4>下载</h4>

<p>通过wget下载最新的saiku社区版，目前版本是3.8.8：</p>

<pre><code>wget www.meteorite.bi/downloads/saiku-latest.zip
</code></pre>

<h4>启动安装注册</h4>

<p>下载后进入saiku-server目录，通过./start-saiku.sh可以启动saiku（同理，通过./stop-saiku.sh关闭saiku）。如果正常，启动后通过你机器的8080端口 (浏览器里visit <a href="http://your_IP:8080">http://your_IP:8080</a>)，可以访问到的saiku的登陆页面。</p>

<p>默认情况下，saiku的管理员账户和密码均是admin，但直接登陆后，会提示：</p>

<pre><code>Could not find license, please get a free license from http://licensing.meteorite.bi. You can upload it at http://server:8080/upload.html
</code></pre>

<p>这种情况是因为目前即使是社区版，也需要在saiku上注册：</p>

<pre><code>http://licensing.meteorite.bi/licenses
</code></pre>

<p>访问上述网址，可能会提示用户登陆，需要先注册用户。
登陆后，可以看到：</p>

<pre><code>LICENCE
    Create new licence
    List all licences
COMPANY
    Create new company
    List all companies
</code></pre>

<p>这样的目录结构，我的情况是，需要先创建一个company，然后再创建licence （company在licence中是必选的），licence的hostname就填写你机器的IP（我没有试过其它是否可以），下载licence文件到本地，然后再通过<a href="http://your_IP:8080/upload.html%E5%B0%86%E8%BF%99%E4%B8%AAlicence%E6%96%87%E4%BB%B6%E6%8F%90%E4%BA%A4%E4%B8%8A%E5%8E%BB%EF%BC%8C%E6%95%B4%E4%B8%AA%E6%B3%A8%E5%86%8C%E8%BF%87%E7%A8%8B%E5%B0%B1%E5%AE%8C%E6%88%90%E4%BA%86%E3%80%82">http://your_IP:8080/upload.html%E5%B0%86%E8%BF%99%E4%B8%AAlicence%E6%96%87%E4%BB%B6%E6%8F%90%E4%BA%A4%E4%B8%8A%E5%8E%BB%EF%BC%8C%E6%95%B4%E4%B8%AA%E6%B3%A8%E5%86%8C%E8%BF%87%E7%A8%8B%E5%B0%B1%E5%AE%8C%E6%88%90%E4%BA%86%E3%80%82</a>
注册完成后，通过admin/admin可以登陆进入saiku。</p>

<h4>端口设置</h4>

<p>默认情况下，saiku启动在8080端口，如果需要更改，可以修改saiku-server/tomcat/conf/server.xml里的port配置，比如把8080全部替换为9090。</p>

<h4>权限控制</h4>

<p>saiku server的权限控制主要通过tomcat设置：</p>

<pre><code>tomcat/webapps/saiku/WEB-INF
</code></pre>

<p>其中applicationContext-saiku.xml会引用applicationContext-spring-security.xml，后者默认会采用jdbc设置，在applicationContext-spring-security-jdbc.xml中配置dataSource：</p>

<pre><code>      &lt;bean id="dataSource"                class="org.springframework.jdbc.datasource.DriverManagerDataSource"&gt;
            &lt;property name="driverClassName" value="com.mysql.jdbc.Driver" /&gt;
            &lt;property name="url"                        value="jdbc:mysql://your_IP:3306/admin" /&gt;
            &lt;property name="username" value="your_name" /&gt;
            &lt;property name="password" value="your_password" /&gt;
    &lt;/bean&gt;
</code></pre>

<h4>数据源连接</h4>

<p>在saiku中，我们需要通过预定义的Cube来指定数据访问。Cube是OLAP中的一个逻辑概念，通过对物理数据表的描述，将数据划分为dimension和metric（在saiku中为measure）。saiku中的Cube通过xml描述，具体含义可以参考<a href="http://mondrian.pentaho.com/documentation/schema.php#Cubes_and_dimensions">这里</a>，后续有机会我会详细介绍。
因此，假设我们已经建立好Cube对应的xml，以及数据库中对应的fact表和dimension表，那么我们还需要告诉saiku如何找到它们。
通过在tomcat/webapps/saiku/WEB-INF/classes/saiku-datasources中新建文件配置：</p>

<pre><code>type=OLAP
name=xxx
driver=mondrian.olap4j.MondrianOlap4jDriver
location=jdbc:mondrian:Jdbc=Jdbc:mysql://xxx/dbname;Catalog=&lt;your_xml_path&gt;
</code></pre>

<p>其中type一般是OLAP，name表示你Cube的名词，driver选择<a href="http://community.pentaho.com/projects/mondrian/">mondrian</a>，它是saiku内部使用的数据处理引擎，支持MDX查询 （从这个角度说，saiku更像是查询平台），location里分别指定xml位置和mysql连接方式。</p>

<p>配置完成后，重启saiku，你需要的数据已导入。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[广告成本（CPA）预测模型初步]]></title>
    <link href="http://yoursite.com/blog/2016/10/23/cpa-perdiction-with-spark/"/>
    <updated>2016-10-23T09:57:40+08:00</updated>
    <id>http://yoursite.com/blog/2016/10/23/cpa-perdiction-with-spark</id>
    <content type="html"><![CDATA[<h2>广告成本（CPA）预测模型初步</h2>

<h3>实验背景</h3>

<p>通过投放管理，我们收集了相当数量广告主的广告投放数据。Facebook广告在设定时包含相当数量的定向信息，最常用的比如国家，性别和年龄。广告投放往往在相同定向上有相似的表现效果（比如同一款游戏投放美国和东南亚，前者的CPA一定比后者高），因此我们可以尝试用历史数据预测广告CPA的未来表现。</p>

<h3>实验目标</h3>

<p>希望依据广告投放的历史数据，构建回归模型，预测广告当前的CPA表现，指导模型投放。</p>

<h3>分析工具</h3>

<p>考虑到数据源直接存储在Hive中，同时也考虑数据量可能的增长预测，我们采用Spark做数据处理和模型预测方面的工作。
Spark本身是基于RDD实现的内存计算框架，它的一个重要应用在于处理机器学习中迭代问题的优化，相对应的库为Spark ml。多说一句的是 ，Spark版本更新较快，相应机器学习库存在ml和mllib两个版本，分别基于DataFrame和RDD处理数据。实际上，DataFame可以视为对RDD的封装，在1.3版本前，它也被称为schema RDD，即包含元数据描述的RDD。
我们选择ml和DataFrame操作数据，因为它们在后续更新的优先级会高于mllib。</p>

<h3>数据源介绍</h3>

<p>我们把离线模块拥有的数据源信息划分为广告维度，产品维度，时间维度，广告特征维度，定向特征维度，创意维度和统计维度这几个部分。
技术上，它们来自Hive表offlin_data，表内容每天会定时加载更新。</p>

<p>广告维度字段 (字段名称我只补充了部分需要说明的，因为大多数字段都是自表意的)
| id | description |
|&mdash;|:&mdash;|:&mdash;:|&mdash;:|
|Bm_id  |Business Manager|
|Account_id|   <br/>
|Campaign_id|  <br/>
|Adset_id| <br/>
|Ad_id|</p>

<p>产品维度</p>

<table>
<thead>
<tr>
<th> id </th>
<th style="text-align:left;"> description </th>
</tr>
</thead>
<tbody>
<tr>
<td>Application_id / promotion_url</td>
<td style="text-align:left;">    App链接</td>
</tr>
<tr>
<td>Product_type</td>
<td style="text-align:left;">  App类型，提取自user_os</td>
</tr>
<tr>
<td>Game_type</td>
<td style="text-align:left;"> 应用类型，需单独处理</td>
</tr>
</tbody>
</table>


<p>时间维度
| id | description |
|&mdash;|:&mdash;|:&mdash;:|&mdash;:|
|Dt / start_dt / end_dt|    数据分析的最小单位是天|</p>

<p>广告特征维度</p>

<table>
<thead>
<tr>
<th> id </th>
<th style="text-align:left;"> description </th>
</tr>
</thead>
<tbody>
<tr>
<td>Bid_amount</td>
<td style="text-align:left;">    广告的出价</td>
</tr>
<tr>
<td>Daily_budget</td>
<td style="text-align:left;">  广告的日预算，如果设置</td>
</tr>
<tr>
<td>Lifetime_budget</td>
<td style="text-align:left;">   广告的生命周期预算，如果设置</td>
</tr>
<tr>
<td>Is_autobid</td>
<td style="text-align:left;">    是否是autobid</td>
</tr>
<tr>
<td>Relevance_score</td>
<td style="text-align:left;">   广告的质量得分</td>
</tr>
<tr>
<td>Page_id</td>
<td style="text-align:left;">   广告使用的page信息</td>
</tr>
</tbody>
</table>


<p>定向特征维度
| id | description |
|&mdash;|:&mdash;|:&mdash;:|&mdash;:|
|Min_age / max_age <br/>
|gender<br/>
|country   <br/>
|city   我们抓取了city信息，如果有的话
|Custom_audience   <br/>
|Connection<br/>
|Exclude_connection<br/>
|Interests <br/>
|…   <br/>
完整的定向特征保持与PE上adset可编辑的定向内容完全一致，这里不一一列出</p>

<p>创意维度
| id | description |
|&mdash;|:&mdash;|:&mdash;:|&mdash;:|
|Video_id / video_image_url |Video的FB ID
|Image_url / image_hash |Image的链接</p>

<p>统计维度
| id | description |
|&mdash;|:&mdash;|:&mdash;:|&mdash;:|
|Impression / reach<br/>
|Clicks<br/>
|Actions   <br/>
|Spend <br/>
|Unique_click/unique_impression</p>

<p>创意分为视频和图片两类，出价方式分为使用autobid和不使用autobid，本次分析只针对图片类创意，不使用autobid的游戏类广告。</p>

<p>Spark中对数据加载通过Hive QL，如下语句：</p>

<pre><code>conf = SparkConf().setAppName("offline-train")
sc = SparkContext(conf=conf)
hc = HiveContext(sc)

# generate your query_sql
df = hc.sql(query_sql)
</code></pre>

<h3>数据预处理</h3>

<p>1.考虑在数据量极小时，CPA的绝对值没有意义（比如一共只有几十美分的花费就带来安装并不能说明说明当前定向下真实成本就是几十美分），因此我们过滤掉单日花费小于20$的广告，因为我们认为在太少的花费下，CPA是不可信的。
2.对autobid的广告，数据源中会把对应的bid_amount设置为0，同时，广告在应用和游戏上通常也有着不同的表现，目前我们没有方法直接判断一个promotion_url是游戏还是应用（后续拟添加支持），但通常应用的出价范围和游戏不一致，因此我们选择bid_amount>50美分的ad作为样本对象。
3.回归中较常用的误差衡量方式是RMSE，一些异常点可能对RMSE的结果影响较大，根据经验，因此我们选择天级别实际cpa小于20的ad作为样本，将cpa大于20的ad视为异常数据。
4.因为我们只分析图片创意，因此我们选择image_url非空的case
5.因为后续OneHotEncoder不接受空字符串，因此把空字符串均转为”unknown”</p>

<p>经过以上过滤后，7天内有效数据量占总记录条数的比例约为2%。</p>

<h3>特征工程</h3>

<p>首先，我们经数据预处理后，从数据源中选择了如下原始特征：</p>

<p>dimensions部分</p>

<pre><code>    Image_url
    Dt
    Age_min
    Age_max
    Is_autobid
    Genders
    Product_type
    Countries
    Bid_amount
    Daily_budget
    Connection_ids
    Custom_audience_ids
    Excluded_connection_ids
    Interest_ids
    Billing_event
</code></pre>

<p>metric部分</p>

<pre><code>    Spend
    Mobile_app_install
</code></pre>

<p>metric部分只用于计算cpa，通过Hive定义的udf完成。</p>

<pre><code>UdfFunc = udf(udf_func, FloatType())
df = df.withColumn(“cpa”, UdfFunc(df[“spend”], df[“mobile_app_install”]))
</code></pre>

<p>1.把age_min和age_max合并为age_avg，然后在最终输入模型的训练特征中去除age_min和age_max：</p>

<pre><code>age_avg = (age_min + age_max) / 2
</code></pre>

<p>2.把dt信息转为weekday，在最终模型中去除dt信息：</p>

<pre><code>def weekday(dt):
    if not dt:
        return '-1'
    import datetime
    d = datetime.datetime.strptime(str(dt), '%Y%m%d')
    return int(d.weekday())
</code></pre>

<p>3.对于如countries, connection_ids，可能在一条记录中包含多个国家或多个connection，我们只抽取其中第一条记录视为记录的代表：</p>

<pre><code>def countries_fill(countries):
    if (not countries) or (countries == 'null'):
        return 'unknown'
    countries = countries.replace('::',',')
    return countries[0]
</code></pre>

<h3>特征编码</h3>

<p>由于Spark.ml的树模型不接受字符串类型输入（与scikit-learn一致），因此我们对所有字符特征都作为string to index的转换，然后做了one hot encoder。</p>

<pre><code># countries -&gt; one-hot encoder, category
countries_indexer = StringIndexer(inputCol="countries", outputCol="countries_index", handleInvalid="skip")
countries_onehot_indexer = OneHotEncoder(dropLast=False, inputCol="countries_index", outputCol="countries_vec")
</code></pre>

<p>Spark.ml支持流水线（pipeline）的数据处理模式，需要我们把相关处理特征加入pipeline中。</p>

<pre><code>stages = [
        countries_indexer, countries_onehot_indexer,
]
# finally
pipeline = Pipeline(stages=stages)
</code></pre>

<p>最终将cpa视为label，将其它特征视为features：</p>

<pre><code>assembler = VectorAssembler(
    inputCols = feature_columns,
    outputCol = "features"
)
stages.append(assembler)
</code></pre>

<h3>模型选择</h3>

<p>在Spark.ml的默认模型中，我们选择随机森林（Random Forest）作为回归预测模型。虽然从效果上讲，Random Forest可能不如其它ensemble方法，但它是强于decision tree的，考虑模型的可解释性，以及实现的便利性，我们直接选择RandomForestRegressor作为回归模型。</p>

<p>对于Random Forest，较为重要的两个参数是单棵树的深度(max depth of tree)和树的个数(num of trees)，对这两个参数通过grid-search确定合理值。</p>

<pre><code># do cross-validation
paramGrid = ParamGridBuilder()\
    .addGrid(random_forest.maxDepth, [3, 5, 7, 9])\
    .addGrid(random_forest.numTrees, [100])\
    .build()
evaluator = RegressionEvaluator()

cv = CrossValidator(estimator=pipeline,
    estimatorParamMaps=paramGrid,
    evaluator=evaluator,
    numFolds=2)
model = cv.fit(df)
print model.avgMetrics
</code></pre>

<p>最终模型选择14天数据为训练数据，1天数据为预测数据，RMSE大约在1.59
预测cpa和实际cpa输出样例（前50条）：</p>

<table>
<thead>
<tr>
<th>        prediction</th>
<th style="text-align:left;">    label</th>
<th style="text-align:center;">            features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1.5217560913620043</td>
<td style="text-align:left;">1.4385713</td>
<td style="text-align:center;">(1042,[0,1,2,3,17&hellip;</td>
</tr>
<tr>
<td>2.3752164632800854</td>
<td style="text-align:left;">5.8022223</td>
<td style="text-align:center;">(1042,[0,1,2,3,19&hellip;</td>
</tr>
<tr>
<td> 2.411657767271753</td>
<td style="text-align:left;">3.0307143</td>
<td style="text-align:center;">(1042,[0,1,2,3,42&hellip;</td>
</tr>
<tr>
<td>3.6899882643509763</td>
<td style="text-align:left;">3.5722387</td>
<td style="text-align:center;">(1042,[0,1,2,3,50&hellip;</td>
</tr>
<tr>
<td> 4.092828817916082</td>
<td style="text-align:left;">2.2818182</td>
<td style="text-align:center;">(1042,[0,1,2,3,55&hellip;</td>
</tr>
<tr>
<td> 4.197697715305045</td>
<td style="text-align:left;"> 3.366192</td>
<td style="text-align:center;">(1042,[0,1,2,3,52&hellip;</td>
</tr>
<tr>
<td> 4.197697715305045</td>
<td style="text-align:left;">4.0157895</td>
<td style="text-align:center;">(1042,[0,1,2,3,46&hellip;</td>
</tr>
<tr>
<td> 4.197697715305045</td>
<td style="text-align:left;">3.4369998</td>
<td style="text-align:center;">(1042,[0,1,2,3,49&hellip;</td>
</tr>
<tr>
<td> 4.071716257839271</td>
<td style="text-align:left;"> 1.917647</td>
<td style="text-align:center;">(1042,[0,1,2,3,44&hellip;</td>
</tr>
<tr>
<td>3.9807423344404818</td>
<td style="text-align:left;">2.6338665</td>
<td style="text-align:center;">(1042,[0,1,2,3,42&hellip;</td>
</tr>
<tr>
<td>3.9807423344404818</td>
<td style="text-align:left;">3.5842857</td>
<td style="text-align:center;">(1042,[0,1,2,3,42&hellip;</td>
</tr>
<tr>
<td> 4.177470410349176</td>
<td style="text-align:left;">1.9453847</td>
<td style="text-align:center;">(1042,[0,1,2,3,42&hellip;</td>
</tr>
<tr>
<td> 4.071716257839271</td>
<td style="text-align:left;">1.8128395</td>
<td style="text-align:center;">(1042,[0,1,2,3,44&hellip;</td>
</tr>
<tr>
<td>1.9035986906299776</td>
<td style="text-align:left;">1.7442857</td>
<td style="text-align:center;">(1042,[0,1,2,3,22&hellip;</td>
</tr>
<tr>
<td>13.808040417768252</td>
<td style="text-align:left;">   16.058</td>
<td style="text-align:center;">(1042,[0,1,2,3,27&hellip;</td>
</tr>
<tr>
<td>14.168610809794092</td>
<td style="text-align:left;">  14.8975</td>
<td style="text-align:center;">(1042,[0,1,2,3,18&hellip;</td>
</tr>
<tr>
<td>11.971184107912041</td>
<td style="text-align:left;">    10.89</td>
<td style="text-align:center;">(1042,[0,1,2,3,19&hellip;</td>
</tr>
<tr>
<td>11.962558073010637</td>
<td style="text-align:left;">     8.89</td>
<td style="text-align:center;">(1042,[0,1,2,3,30&hellip;</td>
</tr>
<tr>
<td>2.3169264529890903</td>
<td style="text-align:left;"> 2.916579</td>
<td style="text-align:center;">(1042,[0,1,2,3,11&hellip;</td>
</tr>
<tr>
<td>2.3169264529890903</td>
<td style="text-align:left;">3.2772727</td>
<td style="text-align:center;">(1042,[0,1,2,3,11&hellip;</td>
</tr>
<tr>
<td>1.5221071103811983</td>
<td style="text-align:left;">6.3782606</td>
<td style="text-align:center;">(1042,[0,1,2,3,11&hellip;</td>
</tr>
<tr>
<td>1.3389387340836876</td>
<td style="text-align:left;">2.1707425</td>
<td style="text-align:center;">(1042,[0,1,2,3,11&hellip;</td>
</tr>
<tr>
<td>1.7074257356107376</td>
<td style="text-align:left;">1.8128985</td>
<td style="text-align:center;">(1042,[0,1,2,3,11&hellip;</td>
</tr>
<tr>
<td>1.8675894765662682</td>
<td style="text-align:left;">3.8840384</td>
<td style="text-align:center;">(1042,[0,1,2,3,11&hellip;</td>
</tr>
<tr>
<td> 2.241272660641193</td>
<td style="text-align:left;">4.9881816</td>
<td style="text-align:center;">(1042,[0,1,2,3,11&hellip;</td>
</tr>
<tr>
<td> 2.268769985708979</td>
<td style="text-align:left;"> 2.851852</td>
<td style="text-align:center;">(1042,[0,1,2,3,11&hellip;</td>
</tr>
<tr>
<td>  2.16831561899445</td>
<td style="text-align:left;">3.3324242</td>
<td style="text-align:center;">(1042,[0,1,2,3,11&hellip;</td>
</tr>
<tr>
<td>  2.16831561899445</td>
<td style="text-align:left;"> 4.152222</td>
<td style="text-align:center;">(1042,[0,1,2,3,11&hellip;</td>
</tr>
<tr>
<td> 2.292079711361142</td>
<td style="text-align:left;">3.9308271</td>
<td style="text-align:center;">(1042,[0,1,2,3,11&hellip;</td>
</tr>
<tr>
<td> 2.292079711361142</td>
<td style="text-align:left;">2.6325426</td>
<td style="text-align:center;">(1042,[0,1,2,3,11&hellip;</td>
</tr>
<tr>
<td>1.8987393163009587</td>
<td style="text-align:left;">2.9423833</td>
<td style="text-align:center;">(1042,[0,1,2,3,11&hellip;</td>
</tr>
<tr>
<td>1.8987393163009587</td>
<td style="text-align:left;">2.2502885</td>
<td style="text-align:center;">(1042,[0,1,2,3,11&hellip;</td>
</tr>
<tr>
<td>2.1668434345072054</td>
<td style="text-align:left;">4.4953847</td>
<td style="text-align:center;">(1042,[0,1,2,3,11&hellip;</td>
</tr>
<tr>
<td>2.1668434345072054</td>
<td style="text-align:left;">2.8725274</td>
<td style="text-align:center;">(1042,[0,1,2,3,11&hellip;</td>
</tr>
<tr>
<td> 2.268769985708979</td>
<td style="text-align:left;"> 2.570139</td>
<td style="text-align:center;">(1042,[0,1,2,3,11&hellip;</td>
</tr>
<tr>
<td> 2.268769985708979</td>
<td style="text-align:left;"> 3.188125</td>
<td style="text-align:center;">(1042,[0,1,2,3,11&hellip;</td>
</tr>
<tr>
<td>  2.16831561899445</td>
<td style="text-align:left;">1.9479818</td>
<td style="text-align:center;">(1042,[0,1,2,3,11&hellip;</td>
</tr>
<tr>
<td>  2.16831561899445</td>
<td style="text-align:left;">3.9414287</td>
<td style="text-align:center;">(1042,[0,1,2,3,11&hellip;</td>
</tr>
<tr>
<td>2.3165876740390625</td>
<td style="text-align:left;"> 2.620024</td>
<td style="text-align:center;">(1042,[0,1,2,3,11&hellip;</td>
</tr>
<tr>
<td> 2.318769978387125</td>
<td style="text-align:left;">1.6355883</td>
<td style="text-align:center;">(1042,[0,1,2,3,11&hellip;</td>
</tr>
<tr>
<td> 2.146834324914294</td>
<td style="text-align:left;">1.5435859</td>
<td style="text-align:center;">(1042,[0,1,2,3,11&hellip;</td>
</tr>
<tr>
<td> 2.146834324914294</td>
<td style="text-align:left;">2.0905683</td>
<td style="text-align:center;">(1042,[0,1,2,3,11&hellip;</td>
</tr>
<tr>
<td> 2.361828624187834</td>
<td style="text-align:left;">2.4541378</td>
<td style="text-align:center;">(1042,[0,1,2,3,11&hellip;</td>
</tr>
<tr>
<td> 2.237440741142636</td>
<td style="text-align:left;">3.7431035</td>
<td style="text-align:center;">(1042,[0,1,2,3,11&hellip;</td>
</tr>
<tr>
<td> 2.237440741142636</td>
<td style="text-align:left;">1.1768421</td>
<td style="text-align:center;">(1042,[0,1,2,3,11&hellip;</td>
</tr>
<tr>
<td>2.2990649947244433</td>
<td style="text-align:left;">  2.50875</td>
<td style="text-align:center;">(1042,[0,1,2,3,11&hellip;</td>
</tr>
<tr>
<td>  2.35518238420236</td>
<td style="text-align:left;"> 3.223077</td>
<td style="text-align:center;">(1042,[0,1,2,3,11&hellip;</td>
</tr>
<tr>
<td>1.9192676385516463</td>
<td style="text-align:left;">3.5867856</td>
<td style="text-align:center;">(1042,[0,1,2,3,11&hellip;</td>
</tr>
<tr>
<td>1.9192676385516463</td>
<td style="text-align:left;">2.2514102</td>
<td style="text-align:center;">(1042,[0,1,2,3,11&hellip;</td>
</tr>
<tr>
<td>2.3522288292714277</td>
<td style="text-align:left;">3.7342856</td>
<td style="text-align:center;">(1042,[0,1,2,3,11&hellip;</td>
</tr>
</tbody>
</table>


<h3>后续展望</h3>

<p>就模型本身而言，在各个方面都还有很大的改进空间：</p>

<pre><code>处理数据集本身的局限性
特征工程的方法
模型的选择和参数调试
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Facebook电商广告投放初步]]></title>
    <link href="http://yoursite.com/blog/2016/10/09/facebook-sale-ads/"/>
    <updated>2016-10-09T10:23:53+08:00</updated>
    <id>http://yoursite.com/blog/2016/10/09/facebook-sale-ads</id>
    <content type="html"><![CDATA[<p>此内容基于一次电商广告投放的介绍。</p>

<p>因为是基于一次介绍的信息，因此内容可能看起来层次感会比较差，但从实践的角度讲，无论是技术还是运营，都可以得到一些帮助。</p>

<p>我们假设广告主在网站埋点的工作已完成，假设对install类广告有一定了解。</p>

<p>Facebook 电商广告分为两类：非DPA广告和DPA广告</p>

<p>创建篇
1.非DPA广告</p>

<pre><code>Campaign：
    选择推广目标为Increase conversions on your website
Adset：
    大部分设置和install一致，区别在于需要首先设置Conversions，Conversions依赖用户在网站的埋点，预定义的行为包括Add To Cart，Purchase等，没有埋点的事件是不可选的，电商常用的事件就是Add To Cart和Purchase。
    投放技巧上，Adset的目标在投放早期最好设置为Add To Cart而不要直接设置为Purchase，因为早期购买数据积累较少，直接设置目标为Purchase很可能不会得到展现。在投放了一两个月后，再设置目标为Purchase。
Ad：
    同样设置方法和Install一致，但从投放技巧上，通常会选择轮播图或者轮播视频，让每个图是不同的产品，后续在统计查看时，找到转化好的产品（爆款），然后集中推广
</code></pre>

<p>2.DPA广告：</p>

<pre><code>Campaign：
    选择推广目标为Promote a product catalog，同时要选择一个Product Catalog

AdSet：
    设置方法不同于Install和非DPA广告，一开始要从Product Catalog里选择一个Product Set，即推广产品的列表。
    在Audience方面，DPA提供了四种默认的Audience方式：
        Viewed or Added to Cart But Not Purchased
        Added to Cart But Not Purchased
        Upsell Products
        Cross-Sell Products
    同时也支持基于View和Add To Cart这些行为去进一步定义custom audience，从这些设置中可以看出，DPA主要是对老客户的reseller，因为它的Audience至少是View product过，而非DPA广告则是对新客户推广。
    在Placement方面，它默认的方式是automatic placement
Ad：
    由于是DPA广告，它在文案等内容上支持插入参数，这些参数来自Product Set的预定义
</code></pre>

<p>统计篇</p>

<pre><code>无论是DPA还是非DPA广告，它们相比Install广告会多Add To Cart和Purchase两个指标，分别包含Add To Cart Number（加入购物车的用户数）和Add To Cart Spend（用户在购物车里加了多少钱），Purchase Number和Purchase Spend。
电商广告主通常优化的唯一目标是ROI，即广告花费spend amounnt相比Purchase Spend的数值。但由于Purchase行为是滞后于花费行为的，因此在投放初期很可能看到只有花费，而没有后续收入的情况，这时依赖产品的ctr和Add to cart spend / spend作为判断依据，即产品ctr较高，以及加入购物车的钱数较多时，后续收回成本的可能性更高。
另外电商相比Install特殊的一点是，因为它经常会在一个广告里放多个产品，因此在Ad级别breakdown数据观察很重要，对于非DPA广告可以观察每个广告图的投放情况，对于DPA广告，可以按Product ID观察。另外，由于Spend是基于Ad而不能细致到每个产品，因此主要观察Purchase和ctr作为产品判断的依据。
</code></pre>

<p>其它技巧及注意</p>

<pre><code>1. 电商广告投放初期可能会有成本回收不及时的问题，因此要有心理预期，回收时间范围会比install更长
2. 考虑先用DPA一般投放，找到爆款后再单独建campaign投放指定产品
3. DPA广告的ROI通常会远高于非DPA广告，因为它投放的是老顾客，相对而言非DPA广告投放的是新顾客
4. 初期设置Add To Cart为目标，后期设置Purchase为目标
5. 不要相信Fb的suggest bid
6. 投放时可以采用小预算，尤其尝试新产品的时候
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[为Django博客添加Elasticsearch搜索]]></title>
    <link href="http://yoursite.com/blog/2016/10/04/django-elasticsearch/"/>
    <updated>2016-10-04T18:56:56+08:00</updated>
    <id>http://yoursite.com/blog/2016/10/04/django-elasticsearch</id>
    <content type="html"><![CDATA[<p>翻翻自己的markdown文件，发现上一篇关于Django的文章已经是大概两年前了。虽然文章没有更新，但由于前一阵开始把博客从Github上迁移回自己写的Django网站，相关工作还是做了一些的，这篇博客算是一篇关于Elasicsearch应用的快手博客。</p>

<p>简介
Elasticsearch是基于Luence开发的实时搜索框架，它提供分布式可扩展的服务，以RESTful API的形式提供对外服务，官网地址在<a href="https://www.elastic.co/products/elasticsearch">这里</a>。</p>

<p>安装
下载地址在<a href="https://www.elastic.co/downloads/elasticsearch">这里</a>，安装步骤也可以按官网设置，启动服务后，Elasticsearch默认通过9200端口对外提供服务，我采用的版本是2.4.1</p>

<p>博客添加搜索
这是应用的主要内容，为了达到这一目的，我们要完成以下几件事：
1.将博客内容添加进Elasticsearch索引
2.在博客内添加搜索访问逻辑</p>

<p>假设我们的博客model如下：</p>

<pre><code>class Blog(models.Model):
    title = models.CharField(max_length=100)
    content = models.TextField()
    pub_date = models.DateField()
    tags = models.ManyToManyField(Tag)
    author = models.ForeignKey(Author, on_delete=models.CASCADE)
</code></pre>

<ol>
<li><p>添加索引
采用Elasticsearch的<a href="https://elasticsearch-py.readthedocs.io/en/master/">Python客户端</a>，创建index名为linpingta-blog,type为article的访问索引，使用bulk方法批量建立索引：</p>

<pre><code>  from elasticsearch import Elasticsearch
  from elasticsearch import helpers
  from blogs.models import Blog

  if __name__ == '__main__':
      es = Elasticsearch()

      # create index
      blogs = Blog.objects.all()
      actions = []
      count = 0
      for blog in blogs:
              print blog.id, blog.title.encode("utf-8")
              action = {
                      "_index": "linpingta-blog",
                      "_type": "article",
                      "_id": blog.id,
                      "_source": {
                              "title": blog.title,
                              "content": blog.content,
                              "author": blog.author.name
                      }
              }
              actions.append(action)
              count = count + 1
              if count &gt; 500:
                      helpers.bulk(es, actions)
                      actions = []
                      count = 0

      if count &gt; 0:
              helpers.bulk(es, actions)
</code></pre>

<p>索引建立完毕后，可以做简单的本地验证：(</p>

<pre><code>  search_word = "python"
  res = es.search(index="linpingta-blog", body={
          "query": {
                  "match":{
                          "content": search_word
                  }
          }
  })
  print "Got %d Hits" % res["hits"]["total"]
  for hit in res['hits']['hits']:
          print ("%(author)s: %(title)s" % hit["_source"]).encode("utf-8")
</code></pre></li>
</ol>


<p>输出结果：</p>

<pre><code>Got 34 Hits
褚桐: Python修饰器
褚桐: excel_convertor
...
</code></pre>

<p>2.博客内添加访问逻辑
在相应页面内添加form，在接受form的views方法里调用Elasticsearch，再将搜索到的博客标题对应到博客，返回给展示页面</p>

<p>blog.html：</p>

<pre><code>&lt;form class="m-blog-search" action="/blogs/search/" method="post"&gt;
    {\% csrf_token \%}
    &lt;input id="search_word" type="text" name="search_word" placeholder="博客标题搜索..."&gt;
    &lt;input type="submit" value="搜索"&gt;
&lt;/form&gt;
</code></pre>

<p> views.py</p>

<pre><code>def search(request):
    if request.method == 'POST':
        search_word = request.POST['search_word']
        es = Elasticsearch()
        res = es.search(index="linpingta-blog", body={
                "query": {
                        "match":{
                                "title": search_word
                        }
                }
        })
        blogs = []
        for hit in res['hits']['hits']:
                title = hit['_source']['title']
                blog = Blog.objects.get(title=title)
                blogs.append(blog)
        context = { 'blogs': blogs }
            return render_to_response('blogs/search_result.html', context=context)
    else:
        return HttpResponseRedirect('/blogs/')
</code></pre>

<p>3.其它
Elasticsearch自带的中文分词功能比较简陋，网上一般采用<a href="https://github.com/medcl/elasticsearch-analysis-ik">ik</a>提供中文搜索的支持。但因为网络原因，配合2.4.1版本的ik一直不能正常下载，考虑时间成本，最后只能暂时先放弃。如文章开头所说，这篇博客只是举了一个Elasticsearch的实例，没有涉及Elasticsearch的原理，这点有待以后有机会再做学习。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[策略-数据应用简介]]></title>
    <link href="http://yoursite.com/blog/2016/09/28/strategy-data-introduction/"/>
    <updated>2016-09-28T16:56:19+08:00</updated>
    <id>http://yoursite.com/blog/2016/09/28/strategy-data-introduction</id>
    <content type="html"><![CDATA[<p>在项目组，除了广告管理（AdManagement），统计更新（Stats Update）和规则应用（Rule Management）这三部分直接与用户交互服务相关的工作外，我其余的时间都主要投入在策略和数据这两个大的方向。后续计划会有一系列文章介绍我在这些方向上究竟做了什么，这篇博客仅仅作为一个总纲，谈谈我对它们的理解。</p>

<p>一. 数据分析, 策略，数据挖掘，数据监控的异同
算是先挖一个大坑吧，首先说，实际上这些概念在很多方面本身并没有明确的界限区分，比如很多招聘里的数据挖掘很可能是数据分析工作，而数据工程师又可能在做报表，所以一定要分清这些概念未必有意义。我之所以在这里拎起这些名词，主要是想把我做的事做归类，因为它们大概可以覆盖我做的策略和数据工作 （下面的内容，也是混合了概念和我具体工作的内容）。</p>

<table>
<thead>
<tr>
<th> 发起者\接收者        </th>
<th style="text-align:center;"> 人  </th>
<th style="text-align:right;">    机器  </th>
</tr>
</thead>
<tbody>
<tr>
<td> 人      </td>
<td style="text-align:center;"> 数据分析 </td>
<td style="text-align:right;">策略 </td>
</tr>
<tr>
<td> 机器      </td>
<td style="text-align:center;"> 数据监控      </td>
<td style="text-align:right;">   数据挖掘 </td>
</tr>
</tbody>
</table>


<p>关于发起者的定义是：数据规则的制定者，或者对数据操作的执行人。
关于接收者的定义是：数据规则的执行者，或者对操作结果的观察者。</p>

<p>1.数据分析：它是一个人到人的操作。
它更多强调的是分析师的价值，通过查看DashBoard（高端的通过BI工具）进而分析数据。分析本身有一些特定的模式（比如漏斗），分析师作为对业务最了解的人，在数据分析中起最重要的作用，更多的是偏宏观和经验的处理问题。
作为技术人员，经常会忽视数据分析的作用（或者认为这个过程没有技术含量），但在实际问题里，数据分析作为最快捷的方式，其实往往是用处最大的，所谓用20%的时间解决80%的问题，数据分析要求业务经验，技术更多的是提供辅助工具：报表，分析工具（同样不要轻视辅助工具，看到看不到，多快看到对业务价值影响很大，BI的生存点主要在此）。
多说一句题外话，我毕业后的第一份工作是在一家BI厂商（Microstrategy），因为在研发中心不会直接接触客户，当时并不了解产品的价值。现在真的到做业务后，才明白好的分析系统对数据应用的价值。</p>

<p>具体到我们的业务，我也有一部分工作在客户数据需求输出上面，为此我开发了一个小的<a href="https://github.com/linpingta/tools/tree/master/task_manager">任务执行框架</a>，减轻一些工作的负担。
我近期在做的是，如何输出一些能够吸引客户的，对客户有用的数据信息。为此我搭建了从FB抓取数据到存储的数据流，也给出了一些业务相关的数据应用方案 （简单举个例子，比如图片质量得分随天变化趋势），但我更想强调的是，在我想法里，如果想吸引用户，必须抓住两点：</p>

<pre><code>1.提供给用户FB现有分析工具不能提供的数据
2.要把有用数据“推”给用户，而不是等用户去查
</code></pre>

<p>这两点提出的原因是，FB的分析工具是很多比我更优秀的工程师开发而成的，虽然我们的系统也有报表，但无论从数据内容和展现效果看都无法相提并论，那么，用户为什么要用我们的报表分析？</p>

<p>第一点，举个例子，ad出价bid_amount是一个状态变量，但它直接影响广告投放给什么人群（CPM），我们去记录bid_amount与CPM的关系，分析竞争情况，这是FB自带分析工具没有的。
第二点，只是把大量数据交给运营是没有意义的，比如广告账户分天的成本，能不能让系统找到里面成本异常的时候，及时告知用户，再多做一步分析。
这两点的内容，是我在这方面工作之后的方向。</p>

<p>2.策略：它是一个人到机器的过程
首先你得熟悉业务。举个例子，在实际管理过大量广告后统计发现：</p>

<pre><code>(1)今天或者过去7天成本达到A的ad必须要关闭
(2)发现某类账户在某些国家下新ad必须要出高于目标1.5倍的价格才能投放，然后投放稳定后价格又要回收到某个价格
(3)广告预算需要缓慢增加，甚至一定时候降低
</code></pre>

<p>这些信息，有些是可以通过理论解释的（比如出价高量级大），有些是不能有理论解释但客观存在的（比如某些账号的成本就会更低），它们必须通过实际业务经营获取，换句话说，就是花钱花出来的。
这些经验，我们称之为规则，可能是一个规则，也可能是一组规则，但最终规则会被表述为策略的方式，交由服务来执行，因此我觉得策略是一个由人到机器的过程，因为它是由人发现总结出来的（广告成本超过A应该停止投放），而由机器去执行的（满足要求停止投放，无论现在是凌晨还是正午）。
但策略又不仅仅是规则，有可能不是人可以观察到的，具体到我做的部分，在线策略主要包括状态机和反馈系统，离线策略主要包括动态参数的调整。这些内容我计划后续会详细介绍，这里只提一个引子：</p>

<p>****状态机：广告有它的生命周期，初始，稳定，衰退，用一个自管理的系统去控制广告投放时，在不同阶段有不同的投放策略。比如出价，初始阶段，可能出价会高一些去获取展现，稳定阶段又要根据实际CPA反馈去调整出价，衰退阶段可能会去尝试保CPM，不行就做关停。这些阶段的定义，ad在阶段间的跳转条件，都是通过状态机实现的</p>

<p>****反馈：由于我们业务的特点（在FB上投放广告，不是RTB的模式，是在FB优化的基础上再做优化），我们不能直接控制每一份流量的出价，而只能去宏观调整ad目标，这种情况下完全准确预测广告出价不太可能，所以我把系统做成一个反馈系统，简单说，看最近一段时间（花费）的成本，高了就调低，低了就调高 （但实际问题里很多时候不是这么简单的，最简单的一个问题，你要是调低的太低花不出去钱了怎么办）</p>

<p>****动态调整：举个例子，对每个广告账户，它能投起来的初始出价是不一样的，我们可以根据经验去规定一个统一的初始倍数，但它未必合适，如果让系统去学习一个账户过去一段时间内的平均出价和CPA关系，再确定一个初始倍数，应该合理的多。</p>

<p>3.数据监控：机器发现一件事情发生，及时通知相关的业务人员
策略不是万能的，而且很多时候是差很远的，因为很多人为的信息并没有告知计算机，因此操作还需要人参与。
举个例子，一个广告的日预算是500$，它花到了，广告被暂停了，那么之后是应该扩大预算继续投放呢，还是保持预算暂停呢？这个问题，可能运营会先看成本，成本低于目标继续投放，高于暂停，但业务的情况是，广告主可能在这个广告定向上有每天投放的配额，这样即使预算花到成本OK，也不能继续投放了。
这些信息，不是机器能够了解的，也不可能交给机器自主决定后续方案。
关于数据监控，对我们的问题，我在如下层次上开发监控：</p>

<p><strong><strong>基础监控：它本身也包括运维类的监控，比如CPU,IO,内存，硬盘等，以及关键字、进程状态监控，很重要，但不是我工作的重点
</strong></strong>业务设置监控：例如用户设置了过高的出价，过小的预算或者过小的定向人群
<strong><strong>业务状态监控：例如账户是否被封禁，创意是否被拒审
</strong></strong>业务效果监控：例如ROI不符合预期，突然剧烈变化</p>

<p>监控部分的输出，我还是认为用户不会主动去看我们的报表，因此从“推”的角度，主要考虑邮件和微信（算是中国特色）两种形式，我剥离了业务本身内容后，相关的代码可见<a href="https://github.com/linpingta/tools/tree/master/task_monitor">这里</a>和<a href="https://github.com/linpingta/weixin-api-related/tree/master/weixin-monitor">这里</a>。</p>

<p>4.数据挖掘：机器到机器的过程
策略和数据挖掘其实很难划分，因为策略也有基于离线数据的处理，数据挖掘也有在线的算法，但前者更简单，可以认为是通过规则把数据分析和监控的结果机器化处理，业务本身的耦合性更强，后者更复杂，通过目标函数的优化去发现一些人无法发现或者解决人无法处理的问题。
这部分的工作，我在项目里去年做过一些尝试，但不太成功，原因首先在于：</p>

<pre><code>对数据挖掘的效果太理想化，认为用和不用能有很明显的区别
</code></pre>

<p>客观来讲主要在于三点：</p>

<pre><code>系统本身数据量不大
A/B测试没法做
业务人员应用意愿不强
</code></pre>

<p>具体来说，第一点是说，我们的广告主不多，反复就这么多数据，区分性不大，第二点是FB业务本身的特点，即使所有设置都一样的两个广告投放，也可能会因为彼此抢量而又不一样的效果，无法像RTB那样按idfa保证A/B测试条件，第三点在于，既然你的系统说不出好，那么业务人员为什么要使用呢？
这部分的教训，我后续应该会专门做下记录。</p>

<p>数据挖掘也是我近期工作的重点，因为我们有了额外的数据源，也有了可评估的历史数据，可以基于Spark做更多的分析，但结果，现在还是未知的。
依据之前在Kaggle的一个比赛，我开发了一个简单的基于python pandas的模型框架，在<a href="https://github.com/linpingta/tools/tree/master/model_trainer">这里</a>。</p>

<p>二. 介绍
说完上述内容，我在这里列下之后博客文章的内容，希望一步步把它们补充完整。</p>

<p>1.自管理系统演化：介绍策略系统的演化过程，包括在线离线部分，以及理想和现实的差距
2.离线数据流建立/应用：介绍离线数据抓取到存储的过程，以及数据分析需求的解决
3.监控内容与方式梳理：业务监控的分类，输出方式，后台服务管理
4.数据挖掘的教训和更新：我做了什么，为什么失败，之后要怎么做</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[google server to server 广告对接]]></title>
    <link href="http://yoursite.com/blog/2016/09/21/google-server-to-server/"/>
    <updated>2016-09-21T17:28:08+08:00</updated>
    <id>http://yoursite.com/blog/2016/09/21/google-server-to-server</id>
    <content type="html"><![CDATA[<p>一. 为什么进行广告对接
互联网广告相比传统广告的优势之一，就是它的效果可以被监控。只有通过监控，如Google这样的大型广告服务公司才能了解广告投放效果，并且进一步按照用户的需求（主要是成本）进行优化。</p>

<p>二. 广告对接形式及问题
最常用的对接形式是嵌入SDK，比如Google和Facebook都有自己的SDK，在应用中添加后，当指定事件发生时，SDK会向服务器汇报事件，作为统计的依据。
另外还有一些第三方的数据监控公司，比如TalkingData, Appflyers，提供数据监控服务。
嵌入SDK主要有三方面的问题：
1. SDK 大小。 应用大小本身对用户体验有直接影响，SDK作为一部分代码添加到应用中，会直接增加应用大小，这是应用开发者或者广告主最为担心的。另一方面，如果多家广告公司（Facebook, Google, 第三方）都有自己的SDK，而应用不得不将它们都添加到应用本身的话，肯定是一个不小的量级，尤其对非游戏类app，这点是难以接受的。
2. 效率。类似第一点，SDK在事件汇报时可能会占用资源，影响用户体验
3. 安全。这点也比较明显，毕竟在自己的产品里嵌入一段不懂的代码，很可能担心它有其他影响</p>

<p>由于上面的问题存在，现在有了一种通过server to server(s2s)方式完成事件汇报的方法。</p>

<p>三. s2s 形式</p>

<p>如名称所示，事件汇报不再是通过SDK由应用（client）直接汇报给广告服务公司（例如Google），而是由广告主自己监控事件后，发送给广告服务公司。
需要说明的是，这里的事件指的是如应用激活或者应用内行为等后续事件，不包括广告点击和展现事件。对于点击展现事件，广告公司可以直接监控到它们。
因此s2s的事件汇报，按另外一种说法，可以称为激活核对。广告公司有所有的点击事件，广告主有所有的激活事件，它们都可以对应到设备ID（对苹果是idfa，对安卓是aaid），下面要做的事就是确定哪些激活来自哪些广告。</p>

<p>假设广告主为甲方，广告公司为乙方，那么按汇报方向划分，s2s可以分为两种形式：
1. 甲->乙: 广告主把自己的所有激活设备ID告知广告公司，广告公司去做点击匹配，返回广告主其中哪些设备ID的激活是来自自己的广告
2. 乙->甲: 广告公司把自己的广告点击ID告知广告主，由广告主匹配激活，返回广告公司哪些激活是来自广告
通常一个设备激活可能会有多个点击存在时，按最后一次点击的广告确认为激活来源广告。</p>

<p>四. Google s2s方式
首先，Google s2s的方式只支持上述“甲->乙”，即广告主汇报事件给Google的形式。
具体而言，向Google的指定网址发送一个Get请求，格式如下：</p>

<pre><code>GET https://www.googleleadservices.com/pagead/conversion/conversion_id/?label=conversion_label&amp;bundleid=xx&amp;idtype=xx&amp;rdid=xx
</code></pre>

<p>具体解释其中的参数：</p>

<pre><code>conversion_id和conversion_label：广告主在Adwords上面创建账户时，会填写这两部分内容，这里的请求中要对应填写，用于确定广告信息（这里我不太确定是否是每个campaign有不同的conversion_id，但我感觉应该是一个广告主身份的确认标志，而不是campaign级别的确认）

bundleid：对于应用而言，就是包名，比如com.example

idtype：确定应用类型，只有2个值可选，分别是idfa和aaid，因为ios和android的广告汇报方式一致，所以在这里告知Google应用的类型

rdid：设备的IDFA（对安卓就是aaid），这里填的是实际值
</code></pre>

<p>这里idfa和aaid都不要加密。
其它可选的输入参数还包括：</p>

<pre><code>appversion：告知app版本信息
value：广告主定义的价值
currency：价值对应的货币单位
</code></pre>

<p>对于上述请求，Google服务器会返回相关信息到广告主指定的网址：</p>

<pre><code>https://广告主网址?advertising_id=ad_id ...
</code></pre>

<p>其中广告主网址需要在Google Adwords中配置，配置后服务器才能将信息发送到网址，返回的参数（当然你也可以理解为对广告主服务器的请求）包括：</p>

<pre><code>ad_id：未加密的android设备ID，或者加密的ios md5
click_url：广告点击网址，包含点击的所有信息
click_ts：广告点击时间
campaign_id, video_id：这里campaign并不是广告活动的ID，而是特指youtube的参数，video也是youtube特有的参数
</code></pre>

<p>在向Google服务器请求时，返回码可能有三种状态：</p>

<pre><code>1.200：表示请求格式正确，但激活并不是来自Google广告
2.302：表示请求格式正确，且激活来自Google广告
3.400：表示请求格式不正确
</code></pre>

<p>五. 与广点通异同
这里有ppt就很清楚了，可惜现在还没有，只能大致说下：</p>

<pre><code>广点通支持 甲-&gt;乙 和 乙-&gt;甲 两种对接方式，Google现在只支持一种
其它区别主要是字段含义方面，但大致信息是一致的
</code></pre>

<p>六. 测试
后续结束时有一个实操测试，测试指定IDFA是否来自Google服务器，但感觉还是骗人的成分多些，实际对接要复杂不少，附Python代码如下：</p>

<pre><code>def get_idfa(idfa):
    try:
        url = "https://test-1470278950694.appspot.com"
        conversion_label = 'abCDEFG12hlJk3Lm4nO'
        conversion_id = ''
        bundleid = 'com.example'
        params = {
            'rdid': idfa,
            'label': conversion_label,
            'bundleid': bundleid,
            'idtype': 'idfa',
        }
        res = requests.get(url, params=params)
        print res.status_code
        return res.status_code
    except Exception as e:
        print 'error', e
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spark常用概念]]></title>
    <link href="http://yoursite.com/blog/2016/09/16/spark-normal-concepts/"/>
    <updated>2016-09-16T10:54:51+08:00</updated>
    <id>http://yoursite.com/blog/2016/09/16/spark-normal-concepts</id>
    <content type="html"><![CDATA[<p>这篇博客主要涵盖了我在Spark学习中遇到的一些较为重要的概念。这些概念对我理解服务运行有很大的帮助，但由于网上有更详细更精彩的解释，我在这里并不打算全面的覆盖每个概念的解释，而主要起到对概念索引的作用。如果需要具体查找相关概念，还请在网上搜索相关名词。</p>

<p>概念列表
1. RDD与DataFrame：RDD是Spark计算的基础单位，它是一个逻辑概念，从程序的角度讲，所有操作都包含在RDD transformation和RDD action之上。DataFrame又称为schema RDD，意为包含元数据定义的RDD，它自1.6版本引入，后续会成为MLlib操作的首选对象。
2. action与transformation：RDD的操作都可以划分为这两类行为。它们的区别在于，transformation的输出是另外一个RDD，action的输出是其它对象或文件。执行上讲，transformation操作并不会立即执行，它是lazy的，Spark只是记录它的执行路径，只有遇到action时，相关操作才会转为physical execution plan具体执行。
3. driver, executor, master, worker：有时候会容易把driver, master或者executor，worker混淆。首先，对Spark程序而言，它只有driver和executor两个概念，driver负责生成tasks，executor负责具体执行。master和worker是cluster manager的概念，cluster manager负责对集群的资源进行调度，因此无论driver还是executor，都是执行在worker node上的，同时driver和master的任务目标也是不同的。
4. cluster manager类别：Spark支持三种集群管理方式：standalone，Mesos和YARN，YARN里面又分为yarn-client和yarn-cluster两种模式。Mesos和YARN都是成熟的集群管理方式，在生产环境里用的更多些，也方便与Hadoop任务贡献资源，standalone是单独的集群，意味对资源的单独占用
5. yarn-client和yarn-cluster的区别：主要在于driver的未知，yarn-client的driver是运行的客户端的，如果客户端关闭，任务就结束了，yarn-cluster的driver是运行在集群ApplicatoinMaster上的，任务运行时不要求客户端active
6. ApplicationMaster是什么：这是YARN的一个概念，所有任务必须通过ApplicationMaster作为容器包装， 它负责与ResourceManager交互，申请资源
7. driver-memory和executor-memory应该如何设置：driver-memory通常不需要太大，因为driver上面不执行任务，但如果有collect操作还是要大一些，一般设置1G即可，executor-memory根据任务实际的需要可以大一些
8. Spark执行参数设置：董有一系列<a href="https://www.iteblog.com/archives/1672">博客</a>，涉及参数设置还有调优，对原理也讲得很多，建议看看
9. DAG：有向无环图，Spark执行任务的基础，每次遇到action时，它会根据依赖，去找所有祖先节点的任务，构成一个DAG后，再作为job执行
10. job,stage和task：相比于RDD是Spark的逻辑概念，job,stage和stask都是Spark物理执行中的概念。它们基本是包含关系，每个job可能会被拆分为多个stage，每个stage里包含多个task。它们都可以在Spark Web UI上查看执行情况。具体而言，每个action操作都会产生一个单独的job，job会根据情况划分stage，主要是根据任务是否可以在本地执行，或者说是否有shuffle，每次shuffle都会划分出两个stage，同一个stage里的任务可以并行执行parallel。
11. shuffle：从底层讲，Spark任何执行也可以划分为map类和reduce类，比如filter,map都是在本地操作，而reduceByKey则需要对key进行归并。每次reduce操作都会引起shuffle，shuffle包括shuffle read和shuffle write，都是对磁盘的操作，因此从性能角度出发，尽可能减少shuffle操作可以提高执行效率
12. Spark Web UI：查看任务执行情况，包括job, stage, storage, task，其中stage里面可以看到执行DAG情况，通常在提交任务机器的4040端口查看，查看只能在任务执行区间，任务结束后不能再查看
13. Spark SQL和Hive QL：区别还是在底层实现框架上，Spark SQL是采用Spark执行，而Hive QL是把任务翻译成map-reduce执行
14. Spark比Hadoop快：并不是绝对的，在某些任务上Spark不一定比Hadoop快。Spark快的主要原因在于：</p>

<pre><code>    1.Spark把任务视为一个DAG执行，在任务间可以并行化，Hadoop是把每个任务作为map-reduce执行，并不会对任务间优化
    2.Spark的任务主要是在内存里计算的，而map-reduce每次都会有shuffle操作，因此对于迭代类任务，大量的读写会较慢
    3.Spark的executor在初始化时启动jvm，后续不需要每次都重新启动，Hadoop的jvm会为每个任务新启动，初始化时间消耗也是一个因素
</code></pre>

<p>15.Spark cache或者persist：默认情况下，每个action都会重新计算它链路上所有的RDD，但如果有RDD被多次重复使用，可以使用rdd.persist()来做缓存，避免重复计算
16.Spark与内存：Spark并不要求把所有数据都放到内存里才能计算</p>

<p>暂时想到的是这么多，在此记录下。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Xgboost-Spark应用调研]]></title>
    <link href="http://yoursite.com/blog/2016/09/01/xgboost-spark/"/>
    <updated>2016-09-01T23:04:15+08:00</updated>
    <id>http://yoursite.com/blog/2016/09/01/xgboost-spark</id>
    <content type="html"><![CDATA[<p>官方文档 <a href="https://spark.apache.org/docs/2.0.0/ml-guide.html">https://spark.apache.org/docs/2.0.0/ml-guide.html</a>
本文假设已正常安装spark和scala，其中spark为standalone模式。</p>

<p>1.官方的介绍资料在<a href="http://dmlc.ml/2016/03/14/xgboost4j-portable-distributed-xgboost-in-spark-flink-and-dataflow.html">这里</a></p>

<p>2.下载地址：<a href="https://github.com/dmlc/xgboost">https://github.com/dmlc/xgboost</a></p>

<p>3.样例运行
(1) 在完成上述下载后，进入git项目根目录，执行make编译源文件生成lib/xgboost.so
    在这一步中，如果遇到宏引用未定义错误，可能需要在对应文件中添加include或者宏定义。
(2) 进入jvm-package目录，执行mvn package编程生成jar包
    不同于spark官网的scala项目管理文件用sbt完成，xgboost项目采用maven完成包管理，如果没有安装maven，需先安装maven2
(3) 运行纯scala示例
    在上一步完成后，jvm-package/xgboost-example/target目录下已经生成了对应的jar文件，进入xgboost-example目录，以BoostFromPrediction为例，执行如下操作：</p>

<pre><code>scala -cp target/xgboost4j-example-0.7-jar-with-dependencies.jar ml.dmlc.xgboost4j.scala.example.BoostFromPrediction
</code></pre>

<p>在这里遇到的问题可能是”scala error: main method is not static“，原因是scala的main函数需要放在object对象下面而不是class对象下面来实现static，因此解决方法是把对应源文件的class对应替换为object并重新编译打包，正常的输出如下:</p>

<pre><code>[0]     train-error:0.046522    test-error:0.042831
[1]     train-error:0.022263    test-error:0.021726
</code></pre>

<p>(4) 运行spark示例
完成上述编译后，在spark目录下提交任务：</p>

<pre><code>./bin/spark-submit --master local[4] --class ml.dmlc.xgboost4j.scala.example.spark.DistTrainWithSpark /home/test/xgboost/jvm-packages/xgboost4j-example/target/xgboost4j-example-0.7-jar-with-dependencies.jar 30 1 /home/test/xgboost/demo/data/agaricus.txt.train /home/test/xgboost/demo/data/agaricus.txt.test /home/test/xgboost/demo/data/result
</code></pre>

<p>xgboost目前只有一个spark示例，上述各项参数的含义在源码中有清晰描述.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[tools相关项目]]></title>
    <link href="http://yoursite.com/blog/2016/08/20/tools-project/"/>
    <updated>2016-08-20T16:20:01+08:00</updated>
    <id>http://yoursite.com/blog/2016/08/20/tools-project</id>
    <content type="html"><![CDATA[<h3>tools</h3>

<p><a href="https://github.com/linpingta/tools">Github项目</a>保存了一些独立的项目和脚本，像项目名称所表示的，它们是用于提高工作效率而开发的，并且可能会在适当的时候被提取划分为独立的项目。</p>

<ol>
<li>project_maker.sh ：快速生成python常用项目的基本目录结构</li>
<li>fabfile.py ：基于<a href="http://www.fabfile.org/">fabric</a>，保存常用的操作，包括git命令和远程目录操作</li>
<li>template_maker ： 用于模板信息的快速填充，基础基于<a href="http://jinja.pocoo.org/">Jinja2</a>，主要是在工作中常用的离线模型和数据监控脚本自动生成</li>
<li>model_trainer ：用于machine learning项目的基本框架，我在参加<a href="https://www.kaggle.com/c/shelter-animal-outcomes">Kaggle竞赛</a>中使用了<a href="https://github.com/linpingta/shelter-animal-outcome">它</a></li>
<li>task_manager ： 用于离线任务的调度，基于DAG执行db任务和用户定义任务</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[离线任务调度框架]]></title>
    <link href="http://yoursite.com/blog/2016/08/19/offline-task-framework/"/>
    <updated>2016-08-19T17:28:08+08:00</updated>
    <id>http://yoursite.com/blog/2016/08/19/offline-task-framework</id>
    <content type="html"><![CDATA[<p>这两天为项目需要写了个查询工具，想想还是有些东西可以提炼出来总结的，所以这篇来描述下我做的事情。</p>

<h2>背景</h2>

<p>首先，我们把广告主的数据做了汇总，最初的数据构建是为系统使用，并不是为用户数据分析使用的（为用户分析和为系统使用的区别，简单举个例子，用户分析关心账户的名称，系统只需要它的ID），但项目里想想既然都花时间做了数据，那索性给用户导出一些分析数据（ps，很理解，但从技术角度讲，导数据的辛苦程度相比做数据也不逞多让，尤其是我们的数据开始并不是为用户准备的，还好我在导出兴趣信息的时候多想想，不仅导出了ID，还导出来兴趣名称…）。</p>

<p>开始的需求是比较模糊的，可能是广告类型的投放分析，国家的投放分析，将来很可能会扩展和定制。</p>

<h2>设计目标</h2>

<p>原则上讲，每个查询需求实际都是对应一个hive查询。如果仅仅是处理目前报告的内容，最直接的方法是理解查询需求，然后写hive脚本，导出结果。</p>

<p>这样做虽然对少量的查询比较方便，但长远考虑，有明显的问题：</p>

<ol>
<li>Hive QL包含大量重复内容，按国家维度可能是查ctr, cpa，按性别维度也是，按年龄维度也是，如果不能保存查询语句，会浪费很多时间重敲语句</li>
<li>查询对用户不友好，因为查询都是要直接写Hive QL完成，一般非技术人员没有能力这样做，最终导致需求全部叠加到技术人员身上</li>
<li>查询效率不高，这里指的不是每条Hive QL语句（好吧为了突出优点有点拼，这里并没有对Hive QL本身的优化，在最后的缺点也会提到），而是多个查询本身是可以并行执行的，如果人工输入却不得不串行执行。尤其认真说，考虑到hive查询本身是比较慢的，等待时间并不十分乐观</li>
<li>Hive数据与导出数据不一致：这点看起来很小，但实际影响很大。虽然有很多查询可以直接使用Hive QL的输出，但还有一些查询需要再对原始数据做进一步的处理。举个例子，我们的国家是按 [A,B,C]保存的，但用户最终要的是国家A，B和C分别保存的信息，忽略或者特殊处理这种合并的情况，直接把hive查询结果返回是不能满足用户需求的</li>
<li>通用操作：比如结果展现给用户的形式，通过web界面，或者通过邮件，对不同的查询是通用的</li>
</ol>


<p>为解决上述问题，我在设计上考虑框架应该实现的几个属性</p>

<ol>
<li>任务化：把每个查询作为一个独立的任务对待，通过db ORM读取再处理，这样做主要是考虑将来可能和用户做交互时，用户的查询需要持久化</li>
<li><p>任务划分：</p>

<p>   Hive查询任务，更通用的说，是和db做交互的任务</p>

<p>   用户定义任务，基于Hive查询任务或其它用户定义任务，做类似ETL或者格式规范化方面的内容</p></li>
<li><p>信息抽取：主要是针对Hive查询任务，对每个查询语句都有select, from, tablename, where等，这些语法信息是用户不关心也无需了解的，让框架来完成这些通用的内容处理，用户只填写相关的信息</p></li>
<li>任务调度：hive查询是一个高IO低CPU的操作，可以通过多线程提高效率。同时由于用户定义任务对其它任务的依赖存在，这个调度变成比较经典的有向无环图（DAG）遍历问题</li>
<li>通用处理：我依赖pandas DataFrame作为数据存储的容器，每个hive查询任务的结果会被转为DataFrame再做持久化，用户依赖任务会依赖DataFrame做进一步操作（的确多了一次落盘读盘，后续考虑改进），DataFrame本身很方便的与csv文件做转换，以及对数据处理的强大支持（嗯还没有内存放不下的数据不用考虑分布式），也会方便后续的处理</li>
</ol>


<h2>框架流程</h2>

<p><img src="http://yoursite.com/images/2016/8/QQ%E6%88%AA%E5%9B%BE20160819173204.jpg" alt="Framework" /></p>

<h2>实现</h2>

<ol>
<li>Task表定义</li>
</ol>


<p>基于Django的ORM定义，Task表里把Hive QL的基本组成部分做了独立的拆分，比如dimension, metric，为方便用户输入，这些字段采用json化的list或者list of dict，这些模块后续会由Task的引擎负责检查和拼装成正确的Hive QL</p>

<pre><code>Class QueryTask(models.Model):

    id = models.IntegerField(primary\_key=True)

    name = models.CharField(max\_length=255, blank=True)

    status = models.IntegerField(blank=True, null=True)

    paused = models.IntegerField(blank=True, null=True)

    dimension = models.CharField(max\_length=1024, blank=True)

    metric = models.CharField(max\_length=1024, blank=True)

    filter = models.CharField(max\_length=1024, blank=True)

    order = models.CharField(max\_length=1024, blank=True)

    tablename = models.CharField(max\_length=1024, blank=True)

    limit = models.IntegerField(blank=True, null=True)

    start\_dt = models.IntegerField(blank=True, null=True)

    end\_dt = models.IntegerField(blank=True, null=True)

    create\_time = models.IntegerField(blank=True, null=True)

parent\_task\_ids = models.CharField(max\_length=45, blank=True)
</code></pre>

<p>后续如果有用户交互的需求，可以把用户的查询保存到db，两边都通过ORM读写。在Django项目外使用modeles.py会有一些小坑，但都不算太大，这里不再描述。</p>

<ol>
<li>任务依赖执行</li>
</ol>


<p>有向无环图的多线程遍历，我依赖的是两个队列，未执行队列放入所有入度为0的任务，已执行队列保存所有执行完毕的任务。子线程负责从未执行队列里读取任务，执行任务，向完成队列里放入任务，主线程负责从已执行队列里取任务，减少对它依赖任务的入度，并将入度为0的任务放入未执行队列。</p>

<p><img src="http://yoursite.com/images/2016/8/QQ%E6%88%AA%E5%9B%BE20160819173500.jpg" alt="Iter" /></p>

<ul>
<li><p>HiveTask</p>

<p>引擎拼装HiveQL</p>

<p>查询执行</p>

<p>结果转换为DataFrame再做dump</p></li>
<li><p>UserDefineTask</p>

<p>加载依赖Task的DataFrame</p>

<p>数据处理，这里采用组合的方式，把任务的实际处理交给Worker来完成</p>

<p>保存数据</p></li>
</ul>


<h2>代码</h2>

<p><a href="https://github.com/linpingta/tools/tree/master/task_manager">Github task_manager</a></p>

<h2>问题</h2>

<p>作为一个粗糙的版本，虽然解决了开始提到的问题，但本身还是有很多的局限性：</p>

<ol>
<li>不支持table join，单个任务的查询数据只能来自一个table，这个是硬伤，对小项目可能还行，但大一点的查询就不够了。想想之前微策略做的就是根据table schema去构建table relationship，然后再封装成db无关的BI查询，还是挺敬仰的。</li>
<li>还是有很多项目耦合的内容在，比如HiveTask可以更一般化到DbTask，读取Task的方法可以考虑db之外更灵活的方式（比如xml），但这个的确是因为时间所限，还是以满足任务需求的扩展为主了。</li>
<li>用户定义任务通用内容的抽取：现在是通过把任务交给worker的方式解耦，但worker里的通用任务还不多，这点在后续的持续应用中应该会得到增加和改善。</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[图表与数据初步调研]]></title>
    <link href="http://yoursite.com/blog/2016/07/21/how-to-select-graph-with-data/"/>
    <updated>2016-07-21T14:24:57+08:00</updated>
    <id>http://yoursite.com/blog/2016/07/21/how-to-select-graph-with-data</id>
    <content type="html"><![CDATA[<p>我们经常说，一图胜千言。即使从广告这个狭义角度看，广告创意中图片好坏对用户的影响远远大于广告文案的影响（这里指的是类似Facebook Newsfeed中title/body的部分，并非图片中的文案），以至于我们在分析时经常采用图片维度，而不是创意维度。
图表的形式本身有很多，常用基本的包括折线图（散点图），饼图，柱状图，复杂的包括Bubble, HeadMap，之前在MicroStrategy的时候可视化应用图片应该更多。。怎么做图是问题的一方面，之前接触过一些d3.js，最近又打算看看python的matplotlib库效果（嗯是对matlab的模拟，所以matlab本身也有很多图可做），但问题的另外一方面：什么场景该用什么样的图表，或者是否不应该用图片来解释数据，是这篇博客想要调研的。</p>

<p>这篇博客的工作背景实际是我在项目里对有权限的Facebook账户做了数据收集，这些数据本来是打算用于模型学习和训练的，但在模型处理之前，我们想输出一些简单的数据结果给用户（另外一方面，模型本身也需要一些data explore），我想借机学习下matplotlib的应用。但是，作为输出给用户的内容，我想除了了解代码上怎么作图之外，同样重要的是该在何种场景下以何种形式表达数据:)</p>

<ol>
<li>表格(Table)：表格并不是一种图，但它却是数据使用非常常用的方式，比如常见的excel表格。表格通常用于包含大量数据和它们的精确值，它的优点在于有助于用户准备的了解数据，但它的缺点也很明显：它不适于表达趋势，不适于比较 （想想90天的股价走势，如果画出折线图可以一眼看出峰谷，如果是表格，再有几位小数）。</li>
<li>折线图(Line)：用于表达趋势，通常横轴是时间，表达某个变量随时间变化发生的变化，比如产品的CPA随天发生的变化。</li>
<li><p>柱状图(Chart/Bar)：用于比较，比如几个国家的CPM，横轴是国家，纵轴是CPM，可以直接看出在不同国家的竞争激烈程度。</p>

<ul>
<li>水平柱状图 vs 垂直柱状图：通常前者更适合表达大小差异</li>
</ul>
</li>
<li>饼图(Pie)：用于表达份额，比如设备只有ios和Android两种，用饼图可以更好的表示二者的比例。相比柱状图，饼图更多的在于表示百分比，而柱状图更适于表示精确值。</li>
<li>区域图(Area)：通常用于表达几个变量的份额随时间变化的情况，比如不同浏览器的占比，如果只用饼图，那么不能表达时间的因素，通过Area可以同时表达总体市场大小以及各个细分取值的占比变化情况</li>
<li>散点图：用于表达数据的相似和分离关系，比如图表上位置相近的点可能有类似的属性，远离的点则相反</li>
<li>气泡图：很像散点图，但它的价值在于通过自身大小增加了一个数据维度，比如同样是使用安卓，投放游戏的产品，用气泡大小表示花费量级</li>
<li>其它图：

<ol>
<li>维恩图：用于表达变量间的共同和私有关系</li>
<li>直方图：变量统计，我能想到的是图像直方图用于表达灰度分布</li>
</ol>
</li>
</ol>


<p>看起来是非常简单的一个介绍（貌似写之前我自己也没有预估），但我想我目前会用到的几种图形场景应该都是涵盖其中了，so，end ~</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[facebook相关项目]]></title>
    <link href="http://yoursite.com/blog/2016/07/20/facebook-related-project/"/>
    <updated>2016-07-20T16:12:16+08:00</updated>
    <id>http://yoursite.com/blog/2016/07/20/facebook-related-project</id>
    <content type="html"><![CDATA[<p>摘自<a href="https://github.com/linpingta/facebook-related">facebook-related READMD.md</a></p>

<p>1.<a href="https://github.com/linpingta/facebook-related/tree/master/facebook-ads-sdk-example">facebook-ads-sdk-example</a>
主要是提炼在项目中用到的一些Facebook API调用实例。Facebook API提供了http的封装实现，也提供了Python和PHP的SDK。在日常项目中主要会用到广告相关的API实例，关于Facebook API类型，可以看<a href="http://linpingta.cn/blog/2016/01/15/facebook-api-related/">这里</a>，在此做一个记录。</p>

<p>2.<a href="https://github.com/linpingta/facebook-related/tree/master/facebook-fan-page-fetcher">facebook-fan-page-fetcher</a>
Facebook主页粉丝的信息抓取，主要依赖CasperJS模拟翻页</p>

<p>3.<a href="https://github.com/linpingta/facebook-related/tree/master/facebook-success-creatives">facebook-success-creatives</a>
爬取Facebook成功创意的信息</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[kaggle-shelter-animal-outcome]]></title>
    <link href="http://yoursite.com/blog/2016/07/10/kaggle-shelter-animal-outcome/"/>
    <updated>2016-07-10T15:31:14+08:00</updated>
    <id>http://yoursite.com/blog/2016/07/10/kaggle-shelter-animal-outcome</id>
    <content type="html"><![CDATA[<p>这是一篇关于<a href="https://www.kaggle.com/c/shelter-animal-outcomes">Kaggle:Shelter Animal Outcomes</a>比赛的总结。</p>

<h3>比赛<a href="https://www.kaggle.com/c/shelter-animal-outcomes">背景</a></h3>

<p>比赛是基于美国一个州过去三年宠物收容所对宠物的记录数据，去预测宠物的最终命运。它本身是一个预测问题，用到的自变量包括官方提供的以下特征：</p>

<pre><code>AnimalID,Name,DateTime,OutcomeType,OutcomeSubtype,AnimalType,SexuponOutcome,AgeuponOutcome,Breed,Color
</code></pre>

<p>去预测宠物的最终命运，可能的选项包括：</p>

<pre><code>Adoption，Died，Euthanasia，Return_to_owner, Transfer
</code></pre>

<p>用到的最终评价标准是<a href="https://www.kaggle.com/c/shelter-animal-outcomes/details/evaluation">multiclass logloss</a>，简单来说就是它需要输出每个测试样本属于每类的可能性，而不是最有可能的类别，同时会更大的惩罚那些肯定性判断 （把每个记录以100%的输出错分会受到很大的惩罚），实际上最终leaderboard中那些误差大于3的提交结果应该都是输出了错误的判决结果。</p>

<h3>比赛结果</h3>

<p>截止本文完成时，我的排名是 54th/1289，比赛还有20天结束，但我已不打算进一步优化结果，最终结果应该会在结束后有所变化，我会在相应时间更新。</p>

<h3>最终方案</h3>

<h4>特征选择和处理</h4>

<p>我最终使用的特征包括：</p>

<ul>
<li>时间特征：DateTime部分被处理为5个部分

<ul>
<li> YEAR/MONTH/WEEKDAY/HOUR/UNIXTIME</li>
</ul>
</li>
<li>年龄特征：原始的年龄特征包含如“3 months”, &ldquo;2 weeks"等，全部转换为天为单位的表示，原始数据中部分动物没有年龄，相应数据被转为0（也看到过一些根据其它信息去预测年龄做填充的方案，没有尝试）</li>
<li>名称特征：原始名称转为是否有名称，名称的长度两个新特征</li>
<li>性别特征：原始性别实际包含Male/Female以及是否被阉割Intact的信息，转为Male/Female和Intact/Not Intact两个新特征</li>
<li>品种特征：首先把训练集和测试集的所有品种做统计，因为记录中每个动物都可能属于多个品种，因此把属于某品种记为1，不属于记为0</li>
<li>颜色特征：处理方法类似品种，和品种的区别是，品种会把是否包含Mix和/作为分隔符，颜色会把空格作为分隔符</li>
<li>去除处理过的原始特征，以及Animal ID信息，对测试集编码后（scikit-learn fit_transform），采用相同的编码方式处理训练集</li>
</ul>


<h4>模型选择</h4>

<p>最终我使用的是由ChenTianqi（中国人，非常厉害）开发的<a href="https://github.com/dmlc/xgboost">XGBoost</a>作为学习模型，如XGBoost名称所示，它本身就是一种boosting方法，在我的测试中，它的效果要比RandomForest更好。</p>

<p>最终的参数：</p>

<pre><code>param = {'max_depth':11, 'eta':0.03, 'subsample':0.75, 'colsample_bytree':0.85, 'eval_metric':'mlogloss', 'objective':'multi:softprob', 'num_class':5, 'verbose':1}
num_round = 400
dtrain = xgb.DMatrix(train_x, label=train_y)
clf = xgb.train(param, dtrain, num_round)
</code></pre>

<p>关于XGBoost的介绍，首先参考它的<a href="https://github.com/dmlc/xgboost/tree/master/python-package">Github</a>（包含使用的Example），其次关于参数调优可以参考<a href="http://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/">这篇文章</a></p>

<p>PS. XGBoost本身是基于C开发的，它提供了R和Python两种接口，就我看到的情况看，R语言的接口相比Python会更方便一些，如果将来有机会，我还会对XGBoost做更多的探索。</p>

<h3>我做过的尝试</h3>

<p>我相信最终的方案并不是这次比赛的全部收获（或者更直白的说，只是很少一部分），作为第一个真正认真参加的Kaggle比赛，我想记录一个Kaggle新手在处理问题的过程，对阅读博客的人有更多帮助。</p>

<p>下面的尝试我大致按时间记录：
1. 第一次：几乎没有处理原始数据，只是去除无用的字段（比如OutcomeSubtype, AnimalID）用RandomForest直接训练，得到的logloss是13.8：这里的问题在于，没有看清题目的损失评价方法是logloss，直接用的分类树而不是回归树，给出一堆0，1结果
2. 第二次：仅仅是更新损失评价方法和树类别，logloss下降到3.4
3. 第三次：开始真正意义上的数据清理，包括如下尝试：</p>

<pre><code>* 处理Age信息，把年月日数据统一为天表示
* 尝试把猫和狗作为独立的对象分别训练
* 处理Datetime，转为年月，weekday以及几点钟
* 添加本地的cross validation
这个阶段结束后，logloss下降到1.8
</code></pre>

<p>4.第四次：回归模型参数，发现在RandomForest中使用的树数量和树深度都太少太浅，调整参数，logloss下降到0.98
5.第五次：增加名称信息，增加Sex分隔处理，处理品种和颜色是否混合的信息
6.第六次：开始使用Grid Search做参数空间的搜索，经过参数优化后，logloss大概在0.82左右
7.第七次：用XGBoost替换RandomForest，调整相关参数，之后又对年龄特征做了细化，logloss达到0.75左右
8.这之后其实有一段很长的时间，logloss并没有什么提高，这期间我尝试过用KNN（没有太多调优，误差在0.98），尝试过<a href="http://linpingta.cn/blog/2016/06/25/model-stacking/#disqus_thread">模型融合</a>，还尝试过使用场外特征，但是都没有在logloss上面有明显的改善（场外还是有一些改善，但并不合法还是放弃了）
9.最后的改进：在特征中加入UnixDateTime特征，以及把XGBoost算法从scikit-learn版本更改为它的原生版本，logloss达到0.72，再更新参数和训练周期(num_round)，最佳logloss达到0.708。</p>

<h3>项目代码</h3>

<p><a href="https://github.com/linpingta/shelter-animal-outcome">Github地址</a></p>

<h3>经验和教训</h3>

<p>大致来说，logloss的改进经过“特征处理->模型调参->特征处理”这样不断循环的过程，在模型大致可行的前提下，特征工程，指的是发现新特征，或者处理原有特征，会对结果又明显的改善，但是这句话的前提是模型大致可行，也就是说模型本身的参数，以及选择哪个模型，仍然是非常重要的。
结论性的来说，模型和特征是解决问题的两个重要因素，哪个弄不动了就试试另外一个，通常会不断提升结果。
这次也对模型融合做了一些尝试，效果不是很好，但相信以后会有更好的结果。</p>

<h3>关于Kaggle</h3>

<p>其实关于kaggle有没有用，网上有很多说法，但主要的说法是，你要指望通过kaggle去做data science，是不太靠谱的。因为它的数据集本身是比较干净的（虽然也有缺省值要处理），问题的目标也是比较清晰的（损失函数都告诉你了嘛），而实际工作中，包括我自己遇到的问题里，如何去定义问题，去处理数据，往往是问题中最困难的（其实定义问题是最困难的，很多问题并没有那么明显的损失函数去描述）。
我觉得这种说法是很有道理的，所以我想说kaggle的比赛结果并不能说明什么，但为什么我还愿意参加这样的比赛，我想是基于以下的理由：</p>

<pre><code>1. 接触那些与工作内容完全不同的问题：Kaggle上有各类机器学习的问题，分类回归聚类，图像NLP，一个人的实际工作中其实很难接触到太多方面，这里是个好途径
2. 学习新的模型，新的方法：恰恰是因为不用去做数据提取和预处理的工作，可以让人更专注于模型本身。我可以学习到新的模型，新的调参方法，这些或许在将来会给予工作帮助
3. 它本身有一定的意义。Kaggle比赛不能排除作弊，但它毕竟是一个很明确的问题。从我不长时间的面试和被面试经验看，机器学习相关的项目经验，由于业务本身特点（内部工具或者多人合作），其实是很难在面试很短时间内描述清楚的。Kaggle的比赛至少可以说明一个人在运用模型方面的基本能力是get了，因为Kaggle的比赛是一定包括训练集测试集特征处理交叉验证这些基础的方法的。我不认为Kaggle的优胜者一定是一个好的data engineer，但至少说明他在数据这个大领域的某些方面还不错。
</code></pre>

<p>最后说一点，除了以上的经验，我根据一般模型训练的步骤开发了一套<a href="https://github.com/linpingta/tools/tree/master/model_trainer">框架</a>，可以方便以后使用。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Celery用于监控微信报警]]></title>
    <link href="http://yoursite.com/blog/2016/07/01/celery-weixin-monitor/"/>
    <updated>2016-07-01T23:47:00+08:00</updated>
    <id>http://yoursite.com/blog/2016/07/01/celery-weixin-monitor</id>
    <content type="html"><![CDATA[<p>应用场景</p>

<p>广告系统会根据业务需求（比如预算到期，创意被拒，账户被封等）给用户发送微信提醒。</p>

<p>之前的实现方法是，由业务模块负责把数据推送到由redis简单构成的消息队列中，然后有一个集中处理模块定期去采集消息队列中的信息，发送给对应用户。</p>

<p>这样实现虽然简单，但是有一些问题的：
1. 定期集中处理相当于是采用轮询的方式查看队列，首先在没有数据时，轮询操作本身是一种浪费资源（尽管在这个场景下浪费不大），其次轮询意味着固定的时间周期，如果时间周期过短肯定是资源消耗增加，如果时间周期过长一方面是报警不及时（你预算都停了半天才告诉我，还不如我自己去看对吧），同时也会让很多报警一次性的发送给用户（微信突然响个不停）
2. redis的队列本身没有重试，也不会保存信息，换句话说，如果采集模块出bug了，这些数据就是丢了，前期报警数据本身没有那么重要（因为不是直接涉及钱的数据），但丢数据从技术角度还是应该解决的问题。
3. 代码复杂度，用redis的话，每个业务服务都得redis客户端对吧，那就要负责开关。每个业务服务通常习惯使用不同的名称（与业务本身相关），这点得和采集模块保持一致，但如果模块多了呢，或者说这种信息定义在业务代理里，本身查看和更新也非常不方便。</p>

<p>Celery可以解决的问题
1.不是同步非阻塞了，是异步查询。执行效率变高了，用户不用等了，如果业务量级大，还是会有延迟，但你可以定义不同优先级的队列处理，用户基本是及时收到报警信息。
2.任务本身可以重试，相关消息也可以根据需要保存。
3.采集模块简化为一个函数，业务调用模块调用函数（实际是发送消息到队列），两侧均不再需要直接接触redis。</p>

<p>另外也研究了Celery用于定时任务管理的功能，考虑对其它语言的接受程度和系统稳定性，还是决定使用公司内部开发的调度系统。关于定时任务的应用，以后有机会再做介绍。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Celery常见问题]]></title>
    <link href="http://yoursite.com/blog/2016/06/28/celery-frequent-asked-question/"/>
    <updated>2016-06-28T09:43:58+08:00</updated>
    <id>http://yoursite.com/blog/2016/06/28/celery-frequent-asked-question</id>
    <content type="html"><![CDATA[<p>问题节选自<a href="http://docs.celeryproject.org/en/latest/faq.html">Frequently Asked Questions</a>，只挑选了一些感觉比较重要的回答，也掺杂了一些自己的理解，有兴趣请阅读<a href="http://docs.celeryproject.org/en/latest/faq.html">原文</a>。</p>

<p>1.Celery的适用场景</p>

<pre><code>(1) 后台运行的场景。比如web应用中，需要快速返回结果给用户，以求得更好的用户体验，但任务本身还需要在后台运行，此时很适合使用Celery。
(2) 周期性执行的任务
</code></pre>

<p>还有几点可以视为Celery的属性</p>

<pre><code>(1) 异步执行，包含重试
(2) 分布式
(3) 并行
</code></pre>

<p>2.Celery的依赖</p>

<p>作者在这里吐槽了下对依赖过多的批评："The rationale behind such a fear is hard to image"，软件本身应该避免重复造轮子，而且有像pip这样的包管理工具，安装依赖本身并不是一件头疼的事情。</p>

<p>这里我想吐槽下依赖的问题：最近在工作里用到Celery发现启动后遇到<a href="http://linpingta.cn/blog/2016/06/24/celery-problem-1/">Socket_connect_timeout错误</a>，最终排查后，发现是kombu的版本略新，而redis的版本略旧导致了问题，所以说依赖如果能够保证版本一致还好，但如果同一模块不同版本间升级较多，依赖者就要麻烦了。</p>

<p>回过头来看看Celery的依赖：</p>

<pre><code>(1) kombu：一个python消息库，据说在openstack项目中也有使用
(2) billard：一个对python自带multiprocessing模块功能升级的模块
(3) pytz：时区管理
</code></pre>

<p>另外Django也提供Celery的使用支持。</p>

<p>3.Celery算是一个轻量级服务吗
是的，但同样需要考虑CPU和IO的[优化]。(<a href="http://docs.celeryproject.org/en/latest/userguide/optimizing.html#guide-optimizing">http://docs.celeryproject.org/en/latest/userguide/optimizing.html#guide-optimizing</a>)</p>

<p>4.Celery的序列化方式
默认是pickle，但同样支持yaml, json等其它格式。</p>

<p>5.Celery适用的web框架
不仅是Django，也包括其它</p>

<p>6.Celery的broker选择
推荐使用RabbitMQ或者支持AMQP协议的消息队列，但是Redis，或者其它数据库（MongoDB, 关系型数据库）也是可以选择的。
补充下，在这里Redis的支持应该是好于其它数据库（无论是关系还是非关系），另外也有文章说最好不要用关系型数据库作为Celery的broker，所以其实主要的选择就是两点：RabbitMQ或者Redis。
像我们工作的环境，RabbitMQ并没有安装，因此我们选择Redis作为broker。</p>

<p>7.Celery支持多语言么
支持
这点我还没有什么应用，这里不乱说了，有兴趣请看<a href="http://docs.celeryproject.org/en/latest/faq.html">原文</a>。</p>

<p>8.如何清除队列里的任务
调用purge命令。
如果是清除所有任务，可以：</p>

<pre><code>celery -A proj purge
</code></pre>

<p>如果是清除指定队列中的任务，可以：</p>

<pre><code>celery -A proj amqp queue.purge &lt;queue name&gt;
</code></pre>

<p>9.如何根据taskID获取任务执行的结果
调用task.AsyncResult命令</p>

<pre><code>result = my_task.AsyncResult(task_id)
result.get()
</code></pre>

<p>10.可以以root身份运行celery么
一定不要这样，可能的安全问题会导致风险。</p>

<p>11.如何在一个任务调用完后执行另外一个任务
依赖<a href="http://docs.celeryproject.org/en/latest/userguide/canvas.html">Canvas</a>中定义的操作符，在执行完A后执行B：</p>

<pre><code>do A | do B
</code></pre>

<p>12.如何取消正在执行的任务</p>

<pre><code>result = add.apply_async(args=[2, 2], countdown=120)
result.revoke()
</code></pre>

<p>13.如何将指定任务发送到指定server
    使用<a href="http://docs.celeryproject.org/en/latest/userguide/routing.html">路由</a></p>

<p>14.Celery支持优先操作么
不支持消息级别的优先级操作，Celery对优先级操作的支持是通过把不同优先级的任务发送到不同的任务路由实现。</p>

<p>15.可以指定时间执行任务么
类似crontab的调用：</p>

<pre><code>from celery.schedules import crontab
from celery.task import periodic_task

@periodic_task(run_every=crontab(hour=7, minute=30, day_of_week="mon"))
def every_monday_morning():
    print("This is run every Monday morning at 7:30")
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[模型融合]]></title>
    <link href="http://yoursite.com/blog/2016/06/25/model-stacking/"/>
    <updated>2016-06-25T11:35:29+08:00</updated>
    <id>http://yoursite.com/blog/2016/06/25/model-stacking</id>
    <content type="html"><![CDATA[<p>这篇博客内容主要参考<a href="http://mlwave.com/kaggle-ensembling-guide/">&ldquo;Kaggle Ensemble Guide&rdquo;</a>，介绍了模型融合方面的一些相关技术（也“融合”我的一些理解）。很推荐直接阅读原文，相信对模型融合的概念会带来更多的了解。</p>

<p>Kaggle Ensembling Guide</p>

<p>对于机器学习相关的任务，模型融合是一种重要的提升学习准确率的方法。所谓融合，可以说是通过将不同模型的学习结果进行综合，使得最终的结果在准确率上超过单个模型的预测结果。</p>

<p>一. 为什么模型融合可以达到这种效果？</p>

<p>举一个简答的例子。假如我们要预测一个0-1序列，序列的真值是：</p>

<pre><code>1111111111
</code></pre>

<p>现在我们有3个预测准确率在70%的模型，分别的预测结果是：</p>

<pre><code>1110001111
1010101111
0111110011
</code></pre>

<p>我们简单的通过多数表决，即对每一位采用最多的结果作为最终结果，可以得到多数表决后的序列：</p>

<pre><code>1110101111
</code></pre>

<p>准确率达到80%对么</p>

<p>看到这里肯定会有一个疑问，这种结果应该也是凑巧吧，如果我的三个模型预测序列是一致的，那么准确率也不会提高。</p>

<p>是这样的，所以在我看来模型融合要求两个基本条件：</p>

<p>1.模型之间是不同的，每个模型会学习到不同的特征，这样融合才有意义，模型融合要求模型彼此从本质上的不同性。
2.模型准确率是相似的。如果把一个80%准确率的模型和一个20%准确率的模型融合，结果肯定还是会变差的，但如果是75%和80%的模型融合，结果很可能是会提高的。</p>

<p>小补充一点，实际应用当中的感觉是，差不多的模型进行容易，一般都会比单个模型有更好的结果。</p>

<p>二. 融合的层次</p>

<p>投票融合： 直接把不同模型的结果。具体的做法又可以分为几种：</p>

<ol>
<li>平均：模型1的结果A1， 模型2的结果A2， 最终结果= (A1+A2)/2。</li>
<li>加权：按不同模型准确率的程度进行加权，越准确的模型有越高的权重，准确率可以通过交叉验证计算。</li>
</ol>


<p>好吧，前面说了那么多模型融合牛逼，最后融合的方法就是这个？
当然不是，但我在这里要说两点：
1. 最简单的平均融合，虽然看起来比较low，但效果并不差，很多时候平均融合就可以取得很不错的效果了。
2. 当然有更高端的融合方式，请接着看下面。</p>

<p>还是先摘自“Kaggle Ensemble Guide”一段很有趣的解释：</p>

<pre><code>Averaging prediction files is nice and easy, but it’s not the only method that the top Kagglers are using. The serious gains start with stacking and blending. Hold on to your top-hats and petticoats: Here be dragons. With 7 heads. Standing on top of 30 other dragons.
</code></pre>

<p>哈哈，再举一个例子，Netflix竞赛算是机器学习比赛的鼻祖了，其中winner的方法就是blending。（ps，还有一个槽点是，Netflix并没有最终使用winner的方法，因为。。在实际中无法使用。pss，这也是Kaggle被吐槽很重要的一个原因，它离实际问题的解决太远了）。</p>

<p>Stack Generalization：首先用一系列分类器对原始数据做分类，然后再用一个分类器去聚合这些分类器分类的结果，总体的降低误差。</p>

<p>Blending：和Stack Generalization很相似，相比Stack，它的好处和坏处可以看<a href="http://mlwave.com/kaggle-ensembling-guide/">原文</a>。</p>

<p>具体是怎么做的呢，举个例子，代码在<a href="https://github.com/emanuele/kaggle_pbr/blob/master/blend.py">这里</a>：</p>

<pre><code>np.random.seed(0) # seed to shuffle the train set

n_folds = 10
verbose = True
shuffle = False

X, y, X_submission = load_data.load()

if shuffle:
    idx = np.random.permutation(y.size)
    X = X[idx]
    y = y[idx]

skf = list(StratifiedKFold(y, n_folds))

clfs = [RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),
        RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),
        ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),
        ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),
        GradientBoostingClassifier(learn_rate=0.05, subsample=0.5, max_depth=6, n_estimators=50)]

print "Creating train and test sets for blending."

dataset_blend_train = np.zeros((X.shape[0], len(clfs)))
dataset_blend_test = np.zeros((X_submission.shape[0], len(clfs)))

for j, clf in enumerate(clfs):
    print j, clf
    dataset_blend_test_j = np.zeros((X_submission.shape[0], len(skf)))
    for i, (train, test) in enumerate(skf):
        print "Fold", i
        X_train = X[train]
        y_train = y[train]
        X_test = X[test]
        y_test = y[test]
        clf.fit(X_train, y_train)
        y_submission = clf.predict_proba(X_test)[:,1]
        dataset_blend_train[test, j] = y_submission
        dataset_blend_test_j[:, i] = clf.predict_proba(X_submission)[:,1]
    dataset_blend_test[:,j] = dataset_blend_test_j.mean(1)

print
print "Blending."
clf = LogisticRegression()
clf.fit(dataset_blend_train, y)
y_submission = clf.predict_proba(dataset_blend_test)[:,1]

print "Linear stretch of predictions to [0,1]"
y_submission = (y_submission - y_submission.min()) / (y_submission.max() - y_submission.min())
</code></pre>

<p>操作步骤：</p>

<p>1.把训练集的数据分组 （即上面的把训练集分为X_train和X_Test）
2.遍历所有model （即上面的clfs遍历），训练model
3.对每个model，预测X_train的结果，把结果存储作为第一层的输出
4.对于测试集的数据采用一样的model预测
5.把第3步的输出作为输入，和最终的输出一起训练第二层的模型
6.用第二层的模型最终预测测试集的结果</p>

<p>博客原文还有很多内容，但我没有看得更多，因此先写到这里，感兴趣的话还是推荐阅读<a href="http://mlwave.com/kaggle-ensembling-guide/">原文</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Celery got an unexpected keyword argument 'socket_connect_timeout']]></title>
    <link href="http://yoursite.com/blog/2016/06/24/celery-problem-1/"/>
    <updated>2016-06-24T10:52:17+08:00</updated>
    <id>http://yoursite.com/blog/2016/06/24/celery-problem-1</id>
    <content type="html"><![CDATA[<p>这是一次问题的错误记录，celery在机器A上可以正常工作，但在机器B上启动时</p>

<pre><code>celery worker -A app --loglevel=info
</code></pre>

<p>会遇到如下问题：</p>

<pre><code>  File "/usr/local/lib/python2.7/site-packages/celery-3.1.23-py2.7.egg/celery/worker/consumer.py", line 376, in connect
    callback=maybe_shutdown,
  File "/usr/local/lib/python2.7/site-packages/kombu-3.0.35-py2.7.egg/kombu/connection.py", line 369, in ensure_connection
    interval_start, interval_step, interval_max, callback)
  File "/usr/local/lib/python2.7/site-packages/kombu-3.0.35-py2.7.egg/kombu/utils/__init__.py", line 246, in retry_over_time
    return fun(*args, **kwargs)
  File "/usr/local/lib/python2.7/site-packages/kombu-3.0.35-py2.7.egg/kombu/connection.py", line 237, in connect
    return self.connection
  File "/usr/local/lib/python2.7/site-packages/kombu-3.0.35-py2.7.egg/kombu/connection.py", line 742, in connection
    self._connection = self._establish_connection()
  File "/usr/local/lib/python2.7/site-packages/kombu-3.0.35-py2.7.egg/kombu/connection.py", line 697, in _establish_connection
    conn = self.transport.establish_connection()
  File "/usr/local/lib/python2.7/site-packages/kombu-3.0.35-py2.7.egg/kombu/transport/virtual/__init__.py", line 809, in establish_connection
    self._avail_channels.append(self.create_channel(self))
  File "/usr/local/lib/python2.7/site-packages/kombu-3.0.35-py2.7.egg/kombu/transport/virtual/__init__.py", line 791, in create_channel
    channel = self.Channel(connection)
  File "/usr/local/lib/python2.7/site-packages/kombu-3.0.35-py2.7.egg/kombu/transport/redis.py", line 464, in __init__
    self.client.info()
  File "/usr/local/lib/python2.7/site-packages/kombu-3.0.35-py2.7.egg/kombu/utils/__init__.py", line 325, in __get__
    value = obj.__dict__[self.__name__] = self.__get(obj)
  File "/usr/local/lib/python2.7/site-packages/kombu-3.0.35-py2.7.egg/kombu/transport/redis.py", line 908, in client
    return self._create_client(async=True)
  File "/usr/local/lib/python2.7/site-packages/kombu-3.0.35-py2.7.egg/kombu/transport/redis.py", line 861, in _create_client
    return self.AsyncClient(connection_pool=self.async_pool)
  File "/usr/local/lib/python2.7/site-packages/kombu-3.0.35-py2.7.egg/kombu/transport/redis.py", line 882, in __init__
    self.connection = self.connection_pool.get_connection('_')
  File "/usr/local/lib/python2.7/site-packages/redis/connection.py", line 455, in get_connection
    connection = self.make_connection()
  File "/usr/local/lib/python2.7/site-packages/redis/connection.py", line 464, in make_connection
    return self.connection_class(**self.connection_kwargs)
TypeError: __init__() got an unexpected keyword argument 'socket_connect_timeout'
</code></pre>

<p>网上关于此问题的主要讨论来自：
<a href="https://github.com/celery/celery/issues/2903">https://github.com/celery/celery/issues/2903</a></p>

<p>但感觉大多数讨论并没有指明问题的解决方法。重新查看错误日志，看到问题最终出现在 /redis/connection.py中，即python redis客户端模块中，其中Connection类的构造函数如下：</p>

<pre><code>class Connection(object):
"Manages TCP communication to and from a Redis server"
description_format = "Connection&lt;host=%(host)s,port=%(port)s,db=%(db)s&gt;"

def __init__(self, host='localhost', port=6379, db=0, password=None,
             socket_timeout=None, encoding='utf-8',
             encoding_errors='strict', decode_responses=False,
             parser_class=DefaultParser):
    self.pid = os.getpid()
    self.host = host
    self.port = port
    self.db = db
    self.password = password
    self.socket_timeout = socket_timeout
    self.encoding = encoding
    self.encoding_errors = encoding_errors
    self.decode_responses = decode_responses
    self._sock = None
    self._parser = parser_class()
</code></pre>

<p>再对比可以运行机器中相应源代码：</p>

<pre><code>class Connection(object):
    "Manages TCP communication to and from a Redis server"
    description_format = "Connection&lt;host=%(host)s,port=%(port)s,db=%(db)s&gt;"

    def __init__(self, host='localhost', port=6379, db=0, password=None,
                 socket_timeout=None, socket_connect_timeout=None,
                 socket_keepalive=False, socket_keepalive_options=None,
                 retry_on_timeout=False, encoding='utf-8',
                 encoding_errors='strict', decode_responses=False,
                 parser_class=DefaultParser, socket_read_size=65536):
        self.pid = os.getpid()
        self.host = host
        self.port = int(port)
        self.db = db
        self.password = password
        self.socket_timeout = socket_timeout
        self.socket_connect_timeout = socket_connect_timeout or socket_timeout
        self.socket_keepalive = socket_keepalive
        self.socket_keepalive_options = socket_keepalive_options or {}
        self.retry_on_timeout = retry_on_timeout
        self.encoding = encoding
        self.encoding_errors = encoding_errors
        self.decode_responses = decode_responses
</code></pre>

<p>可以看到在新版本的redis中才会支持socket_connect_timeout字段。</p>

<p>因为问题的解决办法是：升级redis到2.10以上版本</p>

<pre><code>pip install redis --upgrade
</code></pre>

<p>这个解决方案我也补充在 <a href="https://github.com/celery/celery/issues/2903">https://github.com/celery/celery/issues/2903</a> 的最后了。s</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Weixin相关项目]]></title>
    <link href="http://yoursite.com/blog/2016/06/18/weixin-related-project/"/>
    <updated>2016-06-18T21:11:48+08:00</updated>
    <id>http://yoursite.com/blog/2016/06/18/weixin-related-project</id>
    <content type="html"><![CDATA[<p>1.<a href="https://github.com/linpingta/weixin-api-related/tree/master/wx-api-alpha">wx-api-alpha</a></p>

<pre><code>最开始的设想是想仿照Facebook Ads Python SDK封装一套weixin的API，但实际完成度很低，项目里主要包含的是一个token的自动获取（微信默认每一段时间token就会失效，因此每次调用前需要先确认token有效，这点可以通过token自动获取简化），以及基本requests对http调用的封装。
</code></pre>

<p>2.<a href="https://github.com/linpingta/weixin-api-related/tree/master/dm-message-server/weixin_server">dm-message-server</a></p>

<pre><code>微信对用户输入的交互示例
以及与微信相关的任务管理，包括显示，编辑，创建和删除等
</code></pre>

<p>3.<a href="https://github.com/linpingta/weixin-api-related/tree/master/weixin-monitor">weixin-monitor</a></p>

<pre><code>微信报警模块的简单示例
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Keras深度学习框架(介绍)]]></title>
    <link href="http://yoursite.com/blog/2016/06/08/keras-introduction/"/>
    <updated>2016-06-08T22:14:42+08:00</updated>
    <id>http://yoursite.com/blog/2016/06/08/keras-introduction</id>
    <content type="html"><![CDATA[<p>keras是一个高层次的深度学习模块，底层依赖Theano(默认)或TensorFlow调用，封装了Theano中的功能，简化深度学习相关功能的调用。</p>

<p>Sequential : 用于组合一系列layers，是keras最基础的类。
最常用的网络定义模式如下：</p>

<pre><code>models = Sequential()
models.add(Dense(32, input_dim=784)) # 添加model1
models.add(Activation('relu')) # 添加model2
# 添加其它model
...
# 定义模型
models.compile(optimizer='sgd',loss='categorical_crossentropy', metric=['accuracy'])

# 训练模型
models.fit(train_x, train_y, nb_epoch, batch_size)

# 输出预测
y_pred = models.predict_class(test_x)
</code></pre>

<p>通过非常清晰简单的方式，我们就可以定义一个深度学习网络(models.add)，定义它的优化方式(models.compile)，然后训练模型(models.fit)，最后应用预测(models.predict)。问题的核心仍然在网络构建本身，但通过预定义的model（例如Dense, Activation）使得网络定义本身也得到了简化。</p>

<p>下面本文介绍下keras主要的应用模块。</p>

<p>1.layer本身相关部分</p>

<p>所有layer都有一些公有函数：
(1) get_weights() 返回层中神经元的权值，numpy.array
(2) set_weights() 设置层中神经元的权值
(3) get_config() 返回layer的配置</p>

<p>如果layer是非共享的，那么可以通过</p>

<pre><code>layer.input
layer.output
layer.input_shape
layer.output_shape
</code></pre>

<p>获取层的输入张量，输出张量，输入形状，输出形状。
如果layer是共享的，那么通过 get_xx_at(node_index) 获取指定node的信息。</p>

<p>(1)Dense：较常用的一种Neutral Network layer。</p>

<p>定义</p>

<pre><code>keras.layers.core.Dense(output\_dim, 
init='glorot\_uniform', activation='linear', weights=None ...)

主要参数：
output_dim：输出向量
input_dim：输入向量
activation: 定义传递函数，如果不指定，默认为linear, f(x)=x
</code></pre>

<p>调用实例，输入节点16个，输出节点32个：</p>

<pre><code>model = Sequential()
model.add(Dense(32, input_dim=16))
</code></pre>

<p>(2)Activation: 定义激活函数activation：</p>

<pre><code>keras.layers.core.Activation(activation)
</code></pre>

<p>输入层和输出层维度保持一致。</p>

<p>Dropout: 在模型训练的时候使得部分节点的权重不更新，包含参数p，取值范围是[0,1]，指定一定比例的训练样本不用于节点更新，避免过拟合。</p>

<pre><code>keras.layers.core.Dropout(p)

主要参数：
p：[0, 1], 指定一定比例的样本不用于更新
</code></pre>

<p>其它相关函数可以参考<a href="http://keras.io/layers/core/">这里</a>。</p>

<p>2.卷积相关layer定义
(1)Convolution1(/2/3)D ：卷积操作，分别针对1维、2维和3维的输入操作。</p>

<pre><code>keras.layers.convolutional.Convolution1D(nb_filter, filter_length, init='uniform', activation='linear', weights=None, border_mode='valid', subsample_length=1, W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True, input_dim=None, input_length=None)

主要参数：
nb_filter：指定卷积核的个数（等价于输出的维度）
卷积核的形状：
    一维是长度：filter_length
    二维是形状：nb_row, nb_col
    三维是形状：kernel_dim1/2/3
输入形状：
    一维例如：（10,30），前者是样本数量，后者是每个样本的维度
    二维例如：（3,256,256），前者是样本数量，后面两项是图像的长宽
</code></pre>

<p>(2)MaxPooling(&frac12;/3)D / AveragePooling(&frac12;/3)D：主要解释下pooling，它有点像是降采样，把一定区域内的特征按max或average的方法变为一个特征，用于降低处理的数据量。</p>

<p>3.optimizer</p>

<p>定义keras模型的优化方法，keras支持两种类型的输入，
(1) 指定optimizer类的实例，如：</p>

<pre><code>model = Sequential()
#...
sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='mean_square_error', optimizer=sgd)
</code></pre>

<p>(2) 指定optimizer类的名称，如：</p>

<pre><code>model = Sequential()
model.compile(loss='mean_square_error', optimizer='sgd')
</code></pre>

<p>可选的optimizer方法包括SGD, RMSprop，Adagrad，Adadelta等，这些方法主要的区别在于learning rate的计算，learning rate作为超参数的一种，在早期的神经网络中并不能通过网络本身学习，而是通过使用者的经验（调参）决定，因此learning rate的计算方法本身也是深度学习重要的研究方向之一。简单的理解（或者说以我的理解），learning rate可以采取固定值（经验参数，策略一），也可以采用前期跨度大，后期跨度小（策略二），类似的不同策略衍生出不同的optimizer，更详细的科学解释可见<a href="https://www.quora.com/What-are-differences-between-update-rules-like-AdaDelta-RMSProp-AdaGrad-and-AdaM">这里</a>。</p>

<p>4.objective</p>

<p>定义keras模型的损失函数。同optimizer，keras也支持两种类型的输入：
(1) objective的名称，常用的如：</p>

<pre><code>mean_square_error/mse
binary_crossentropy
categorical_crossentropy
</code></pre>

<p>(2) Theano/TensorFlow的函数，类似<a href="https://github.com/fchollet/keras/blob/master/keras/objectives.py">如下</a>：</p>

<pre><code>def mean_squared_error(y_true, y_pred):
    return K.mean(K.square(y_pred - y_true), axis=-1)
</code></pre>

<p>5.activation</p>

<p>神经网络中的激活函数（=转移函数），常用的比如：</p>

<pre><code>model = Sequential()
model.add(Dense(64, activation='tanh'))
</code></pre>

<p>添加了tanh作为激活函数。这里同样可以传入函数对象：</p>

<pre><code>def tanh(x):
    return K.tanh(x)

model.add(Dense(64, activation=tanh))
model.add(Activation(tanh))
</code></pre>

<p>6.initializations</p>

<p>网络层的初始权值，不同层可以定义不同的权值分布，例如：</p>

<pre><code>model.add(Dense(64, init='uniform'))
</code></pre>

<p>定义一个包含64个节点，初始权值一致的网络层。</p>

<p>7.compile函数
objective和optimizer是compile函数必须的两项输入参数，例如：</p>

<pre><code>model = Sequential()
model.compile(loss='mean_square_error', optimizer='sgd')
</code></pre>
]]></content>
  </entry>
  
</feed>
